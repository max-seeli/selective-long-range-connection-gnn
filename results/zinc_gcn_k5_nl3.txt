nohup: ignoring input

Creating k-hop graphs:   0%|          | 0/12000 [00:00<?, ?graphs/s]
Creating k-hop graphs:   0%|          | 35/12000 [00:00<00:35, 341.18graphs/s]
Creating k-hop graphs:   1%|          | 73/12000 [00:00<00:34, 350.08graphs/s]
Creating k-hop graphs:   1%|          | 111/12000 [00:00<00:32, 362.05graphs/s]
Creating k-hop graphs:   1%|          | 148/12000 [00:00<00:32, 360.72graphs/s]
Creating k-hop graphs:   2%|▏         | 187/12000 [00:00<00:31, 369.60graphs/s]
Creating k-hop graphs:   2%|▏         | 224/12000 [00:00<00:33, 355.99graphs/s]
Creating k-hop graphs:   2%|▏         | 261/12000 [00:00<00:32, 357.28graphs/s]
Creating k-hop graphs:   3%|▎         | 301/12000 [00:00<00:31, 367.32graphs/s]
Creating k-hop graphs:   3%|▎         | 342/12000 [00:00<00:30, 377.49graphs/s]
Creating k-hop graphs:   3%|▎         | 380/12000 [00:01<00:32, 358.84graphs/s]
Creating k-hop graphs:   3%|▎         | 417/12000 [00:01<00:32, 357.78graphs/s]
Creating k-hop graphs:   4%|▍         | 453/12000 [00:01<00:32, 352.36graphs/s]
Creating k-hop graphs:   4%|▍         | 491/12000 [00:01<00:32, 358.95graphs/s]
Creating k-hop graphs:   4%|▍         | 527/12000 [00:01<00:32, 353.88graphs/s]
Creating k-hop graphs:   5%|▍         | 564/12000 [00:01<00:31, 358.01graphs/s]
Creating k-hop graphs:   5%|▌         | 600/12000 [00:01<00:32, 347.20graphs/s]
Creating k-hop graphs:   5%|▌         | 638/12000 [00:01<00:31, 355.88graphs/s]
Creating k-hop graphs:   6%|▌         | 674/12000 [00:01<00:32, 349.66graphs/s]
Creating k-hop graphs:   6%|▌         | 714/12000 [00:01<00:31, 361.98graphs/s]
Creating k-hop graphs:   6%|▋         | 751/12000 [00:02<00:32, 351.51graphs/s]
Creating k-hop graphs:   7%|▋         | 792/12000 [00:02<00:30, 368.24graphs/s]
Creating k-hop graphs:   7%|▋         | 829/12000 [00:02<00:31, 360.01graphs/s]
Creating k-hop graphs:   7%|▋         | 866/12000 [00:02<00:31, 354.67graphs/s]
Creating k-hop graphs:   8%|▊         | 902/12000 [00:02<00:31, 355.58graphs/s]
Creating k-hop graphs:   8%|▊         | 938/12000 [00:02<00:31, 346.84graphs/s]
Creating k-hop graphs:   8%|▊         | 975/12000 [00:02<00:31, 351.07graphs/s]
Creating k-hop graphs:   8%|▊         | 1011/12000 [00:02<00:31, 349.28graphs/s]
Creating k-hop graphs:   9%|▉         | 1054/12000 [00:02<00:29, 371.17graphs/s]
Creating k-hop graphs:   9%|▉         | 1092/12000 [00:03<00:29, 367.63graphs/s]
Creating k-hop graphs:   9%|▉         | 1132/12000 [00:03<00:28, 376.26graphs/s]
Creating k-hop graphs:  10%|▉         | 1170/12000 [00:03<00:29, 367.36graphs/s]
Creating k-hop graphs:  10%|█         | 1207/12000 [00:03<00:30, 348.44graphs/s]
Creating k-hop graphs:  10%|█         | 1243/12000 [00:03<00:32, 333.73graphs/s]
Creating k-hop graphs:  11%|█         | 1278/12000 [00:03<00:31, 335.10graphs/s]
Creating k-hop graphs:  11%|█         | 1319/12000 [00:03<00:30, 353.69graphs/s]
Creating k-hop graphs:  11%|█▏        | 1355/12000 [00:03<00:29, 355.26graphs/s]
Creating k-hop graphs:  12%|█▏        | 1391/12000 [00:03<00:30, 346.17graphs/s]
Creating k-hop graphs:  12%|█▏        | 1426/12000 [00:04<00:31, 339.76graphs/s]
Creating k-hop graphs:  12%|█▏        | 1461/12000 [00:04<00:30, 341.89graphs/s]
Creating k-hop graphs:  12%|█▏        | 1498/12000 [00:04<00:30, 348.38graphs/s]
Creating k-hop graphs:  13%|█▎        | 1533/12000 [00:04<00:31, 336.42graphs/s]
Creating k-hop graphs:  13%|█▎        | 1568/12000 [00:04<00:30, 338.90graphs/s]
Creating k-hop graphs:  13%|█▎        | 1605/12000 [00:04<00:29, 347.58graphs/s]
Creating k-hop graphs:  14%|█▎        | 1643/12000 [00:04<00:29, 353.30graphs/s]
Creating k-hop graphs:  14%|█▍        | 1681/12000 [00:04<00:28, 360.90graphs/s]
Creating k-hop graphs:  14%|█▍        | 1721/12000 [00:04<00:27, 372.18graphs/s]
Creating k-hop graphs:  15%|█▍        | 1759/12000 [00:04<00:27, 373.20graphs/s]
Creating k-hop graphs:  15%|█▍        | 1797/12000 [00:05<00:27, 371.19graphs/s]
Creating k-hop graphs:  15%|█▌        | 1835/12000 [00:05<00:28, 361.24graphs/s]
Creating k-hop graphs:  16%|█▌        | 1874/12000 [00:05<00:27, 366.25graphs/s]
Creating k-hop graphs:  16%|█▌        | 1911/12000 [00:05<00:27, 365.73graphs/s]
Creating k-hop graphs:  16%|█▌        | 1948/12000 [00:05<00:28, 350.46graphs/s]
Creating k-hop graphs:  17%|█▋        | 1984/12000 [00:05<00:28, 348.98graphs/s]
Creating k-hop graphs:  17%|█▋        | 2019/12000 [00:05<00:28, 347.94graphs/s]
Creating k-hop graphs:  17%|█▋        | 2054/12000 [00:05<00:28, 346.19graphs/s]
Creating k-hop graphs:  17%|█▋        | 2089/12000 [00:05<00:28, 344.91graphs/s]
Creating k-hop graphs:  18%|█▊        | 2124/12000 [00:05<00:28, 342.30graphs/s]
Creating k-hop graphs:  18%|█▊        | 2159/12000 [00:06<00:29, 336.27graphs/s]
Creating k-hop graphs:  18%|█▊        | 2193/12000 [00:06<00:29, 329.54graphs/s]
Creating k-hop graphs:  19%|█▊        | 2226/12000 [00:06<00:29, 328.67graphs/s]
Creating k-hop graphs:  19%|█▉        | 2261/12000 [00:06<00:29, 334.19graphs/s]
Creating k-hop graphs:  19%|█▉        | 2296/12000 [00:06<00:28, 336.73graphs/s]
Creating k-hop graphs:  19%|█▉        | 2331/12000 [00:06<00:28, 337.97graphs/s]
Creating k-hop graphs:  20%|█▉        | 2365/12000 [00:06<00:28, 335.77graphs/s]
Creating k-hop graphs:  20%|██        | 2407/12000 [00:06<00:26, 356.34graphs/s]
Creating k-hop graphs:  20%|██        | 2443/12000 [00:06<00:27, 353.34graphs/s]
Creating k-hop graphs:  21%|██        | 2479/12000 [00:07<00:27, 352.12graphs/s]
Creating k-hop graphs:  21%|██        | 2518/12000 [00:07<00:26, 361.34graphs/s]
Creating k-hop graphs:  21%|██▏       | 2555/12000 [00:07<00:28, 330.18graphs/s]
Creating k-hop graphs:  22%|██▏       | 2592/12000 [00:07<00:27, 340.09graphs/s]
Creating k-hop graphs:  22%|██▏       | 2633/12000 [00:07<00:26, 356.28graphs/s]
Creating k-hop graphs:  22%|██▏       | 2671/12000 [00:07<00:25, 359.16graphs/s]
Creating k-hop graphs:  23%|██▎       | 2710/12000 [00:07<00:25, 366.76graphs/s]
Creating k-hop graphs:  23%|██▎       | 2747/12000 [00:07<00:25, 359.49graphs/s]
Creating k-hop graphs:  23%|██▎       | 2784/12000 [00:07<00:26, 343.11graphs/s]
Creating k-hop graphs:  24%|██▎       | 2821/12000 [00:08<00:26, 348.67graphs/s]
Creating k-hop graphs:  24%|██▍       | 2857/12000 [00:08<00:26, 350.97graphs/s]
Creating k-hop graphs:  24%|██▍       | 2893/12000 [00:08<00:25, 353.33graphs/s]
Creating k-hop graphs:  24%|██▍       | 2929/12000 [00:08<00:26, 343.61graphs/s]
Creating k-hop graphs:  25%|██▍       | 2968/12000 [00:08<00:25, 355.69graphs/s]
Creating k-hop graphs:  25%|██▌       | 3006/12000 [00:08<00:24, 361.98graphs/s]
Creating k-hop graphs:  25%|██▌       | 3043/12000 [00:08<00:25, 356.63graphs/s]
Creating k-hop graphs:  26%|██▌       | 3079/12000 [00:08<00:25, 353.91graphs/s]
Creating k-hop graphs:  26%|██▌       | 3115/12000 [00:08<00:25, 348.57graphs/s]
Creating k-hop graphs:  26%|██▋       | 3152/12000 [00:08<00:25, 353.81graphs/s]
Creating k-hop graphs:  27%|██▋       | 3188/12000 [00:09<00:24, 354.89graphs/s]
Creating k-hop graphs:  27%|██▋       | 3224/12000 [00:09<00:24, 353.18graphs/s]
Creating k-hop graphs:  27%|██▋       | 3262/12000 [00:09<00:24, 357.76graphs/s]
Creating k-hop graphs:  28%|██▊       | 3301/12000 [00:09<00:23, 366.11graphs/s]
Creating k-hop graphs:  28%|██▊       | 3338/12000 [00:09<00:24, 350.65graphs/s]
Creating k-hop graphs:  28%|██▊       | 3377/12000 [00:09<00:23, 361.06graphs/s]
Creating k-hop graphs:  28%|██▊       | 3414/12000 [00:09<00:23, 363.18graphs/s]
Creating k-hop graphs:  29%|██▉       | 3455/12000 [00:09<00:22, 376.54graphs/s]
Creating k-hop graphs:  29%|██▉       | 3493/12000 [00:09<00:23, 368.01graphs/s]
Creating k-hop graphs:  29%|██▉       | 3535/12000 [00:09<00:22, 382.13graphs/s]
Creating k-hop graphs:  30%|██▉       | 3574/12000 [00:10<00:23, 360.33graphs/s]
Creating k-hop graphs:  30%|███       | 3612/12000 [00:10<00:23, 363.81graphs/s]
Creating k-hop graphs:  30%|███       | 3649/12000 [00:10<00:23, 360.34graphs/s]
Creating k-hop graphs:  31%|███       | 3686/12000 [00:10<00:23, 354.45graphs/s]
Creating k-hop graphs:  31%|███       | 3722/12000 [00:10<00:24, 338.65graphs/s]
Creating k-hop graphs:  31%|███▏      | 3759/12000 [00:10<00:23, 346.13graphs/s]
Creating k-hop graphs:  32%|███▏      | 3794/12000 [00:10<00:24, 338.79graphs/s]
Creating k-hop graphs:  32%|███▏      | 3829/12000 [00:10<00:24, 335.64graphs/s]
Creating k-hop graphs:  32%|███▏      | 3865/12000 [00:10<00:23, 342.49graphs/s]
Creating k-hop graphs:  33%|███▎      | 3901/12000 [00:11<00:23, 346.21graphs/s]
Creating k-hop graphs:  33%|███▎      | 3937/12000 [00:11<00:23, 347.19graphs/s]
Creating k-hop graphs:  33%|███▎      | 3973/12000 [00:11<00:23, 348.93graphs/s]
Creating k-hop graphs:  33%|███▎      | 4010/12000 [00:11<00:22, 351.24graphs/s]
Creating k-hop graphs:  34%|███▎      | 4046/12000 [00:11<00:23, 340.49graphs/s]
Creating k-hop graphs:  34%|███▍      | 4081/12000 [00:11<00:23, 335.30graphs/s]
Creating k-hop graphs:  34%|███▍      | 4116/12000 [00:11<00:23, 339.40graphs/s]
Creating k-hop graphs:  35%|███▍      | 4155/12000 [00:11<00:22, 353.53graphs/s]
Creating k-hop graphs:  35%|███▍      | 4193/12000 [00:11<00:21, 359.51graphs/s]
Creating k-hop graphs:  35%|███▌      | 4233/12000 [00:11<00:20, 370.66graphs/s]
Creating k-hop graphs:  36%|███▌      | 4271/12000 [00:12<00:21, 353.82graphs/s]
Creating k-hop graphs:  36%|███▌      | 4307/12000 [00:12<00:22, 348.98graphs/s]
Creating k-hop graphs:  36%|███▌      | 4343/12000 [00:12<00:21, 349.29graphs/s]
Creating k-hop graphs:  37%|███▋      | 4381/12000 [00:12<00:21, 356.36graphs/s]
Creating k-hop graphs:  37%|███▋      | 4417/12000 [00:12<00:25, 302.16graphs/s]
Creating k-hop graphs:  37%|███▋      | 4457/12000 [00:12<00:23, 325.87graphs/s]
Creating k-hop graphs:  37%|███▋      | 4491/12000 [00:12<00:22, 328.65graphs/s]
Creating k-hop graphs:  38%|███▊      | 4528/12000 [00:12<00:22, 337.67graphs/s]
Creating k-hop graphs:  38%|███▊      | 4563/12000 [00:12<00:22, 330.76graphs/s]
Creating k-hop graphs:  38%|███▊      | 4603/12000 [00:13<00:21, 349.44graphs/s]
Creating k-hop graphs:  39%|███▊      | 4642/12000 [00:13<00:20, 359.47graphs/s]
Creating k-hop graphs:  39%|███▉      | 4679/12000 [00:13<00:20, 349.93graphs/s]
Creating k-hop graphs:  39%|███▉      | 4715/12000 [00:13<00:20, 350.57graphs/s]
Creating k-hop graphs:  40%|███▉      | 4753/12000 [00:13<00:20, 358.03graphs/s]
Creating k-hop graphs:  40%|███▉      | 4789/12000 [00:13<00:20, 352.57graphs/s]
Creating k-hop graphs:  40%|████      | 4829/12000 [00:13<00:19, 365.94graphs/s]
Creating k-hop graphs:  41%|████      | 4866/12000 [00:13<00:20, 353.74graphs/s]
Creating k-hop graphs:  41%|████      | 4902/12000 [00:13<00:20, 351.81graphs/s]
Creating k-hop graphs:  41%|████      | 4938/12000 [00:14<00:20, 347.54graphs/s]
Creating k-hop graphs:  41%|████▏     | 4974/12000 [00:14<00:20, 349.69graphs/s]
Creating k-hop graphs:  42%|████▏     | 5010/12000 [00:14<00:20, 344.50graphs/s]
Creating k-hop graphs:  42%|████▏     | 5046/12000 [00:14<00:20, 347.51graphs/s]
Creating k-hop graphs:  42%|████▏     | 5081/12000 [00:14<00:20, 344.73graphs/s]
Creating k-hop graphs:  43%|████▎     | 5116/12000 [00:14<00:20, 338.61graphs/s]
Creating k-hop graphs:  43%|████▎     | 5150/12000 [00:14<00:20, 332.45graphs/s]
Creating k-hop graphs:  43%|████▎     | 5184/12000 [00:14<00:21, 310.51graphs/s]
Creating k-hop graphs:  44%|████▎     | 5220/12000 [00:14<00:20, 323.23graphs/s]
Creating k-hop graphs:  44%|████▍     | 5253/12000 [00:14<00:20, 324.43graphs/s]
Creating k-hop graphs:  44%|████▍     | 5292/12000 [00:15<00:19, 338.34graphs/s]
Creating k-hop graphs:  44%|████▍     | 5331/12000 [00:15<00:19, 347.92graphs/s]
Creating k-hop graphs:  45%|████▍     | 5368/12000 [00:15<00:18, 352.94graphs/s]
Creating k-hop graphs:  45%|████▌     | 5404/12000 [00:15<00:18, 348.75graphs/s]
Creating k-hop graphs:  45%|████▌     | 5441/12000 [00:15<00:18, 353.13graphs/s]
Creating k-hop graphs:  46%|████▌     | 5477/12000 [00:15<00:19, 340.45graphs/s]
Creating k-hop graphs:  46%|████▌     | 5512/12000 [00:15<00:19, 338.50graphs/s]
Creating k-hop graphs:  46%|████▌     | 5548/12000 [00:15<00:18, 343.06graphs/s]
Creating k-hop graphs:  47%|████▋     | 5584/12000 [00:15<00:18, 346.24graphs/s]
Creating k-hop graphs:  47%|████▋     | 5620/12000 [00:16<00:18, 349.81graphs/s]
Creating k-hop graphs:  47%|████▋     | 5660/12000 [00:16<00:17, 363.28graphs/s]
Creating k-hop graphs:  47%|████▋     | 5697/12000 [00:16<00:17, 357.07graphs/s]
Creating k-hop graphs:  48%|████▊     | 5733/12000 [00:16<00:17, 354.91graphs/s]
Creating k-hop graphs:  48%|████▊     | 5770/12000 [00:16<00:17, 357.43graphs/s]
Creating k-hop graphs:  48%|████▊     | 5806/12000 [00:16<00:17, 355.44graphs/s]
Creating k-hop graphs:  49%|████▊     | 5842/12000 [00:16<00:17, 351.59graphs/s]
Creating k-hop graphs:  49%|████▉     | 5878/12000 [00:16<00:17, 345.59graphs/s]
Creating k-hop graphs:  49%|████▉     | 5913/12000 [00:16<00:17, 340.25graphs/s]
Creating k-hop graphs:  50%|████▉     | 5949/12000 [00:16<00:17, 342.68graphs/s]
Creating k-hop graphs:  50%|████▉     | 5987/12000 [00:17<00:17, 353.13graphs/s]
Creating k-hop graphs:  50%|█████     | 6024/12000 [00:17<00:16, 356.78graphs/s]
Creating k-hop graphs:  51%|█████     | 6062/12000 [00:17<00:16, 361.91graphs/s]
Creating k-hop graphs:  51%|█████     | 6105/12000 [00:17<00:15, 377.57graphs/s]
Creating k-hop graphs:  51%|█████     | 6143/12000 [00:17<00:16, 354.98graphs/s]
Creating k-hop graphs:  51%|█████▏    | 6179/12000 [00:17<00:17, 332.60graphs/s]
Creating k-hop graphs:  52%|█████▏    | 6216/12000 [00:17<00:16, 342.76graphs/s]
Creating k-hop graphs:  52%|█████▏    | 6251/12000 [00:17<00:17, 336.27graphs/s]
Creating k-hop graphs:  52%|█████▏    | 6287/12000 [00:17<00:16, 339.89graphs/s]
Creating k-hop graphs:  53%|█████▎    | 6324/12000 [00:18<00:16, 344.15graphs/s]
Creating k-hop graphs:  53%|█████▎    | 6360/12000 [00:18<00:16, 348.34graphs/s]
Creating k-hop graphs:  53%|█████▎    | 6395/12000 [00:18<00:16, 339.28graphs/s]
Creating k-hop graphs:  54%|█████▎    | 6430/12000 [00:18<00:16, 338.82graphs/s]
Creating k-hop graphs:  54%|█████▍    | 6464/12000 [00:18<00:16, 337.65graphs/s]
Creating k-hop graphs:  54%|█████▍    | 6502/12000 [00:18<00:15, 348.92graphs/s]
Creating k-hop graphs:  54%|█████▍    | 6537/12000 [00:18<00:16, 340.86graphs/s]
Creating k-hop graphs:  55%|█████▍    | 6576/12000 [00:18<00:15, 353.99graphs/s]
Creating k-hop graphs:  55%|█████▌    | 6612/12000 [00:18<00:15, 349.54graphs/s]
Creating k-hop graphs:  55%|█████▌    | 6648/12000 [00:18<00:15, 343.96graphs/s]
Creating k-hop graphs:  56%|█████▌    | 6683/12000 [00:19<00:15, 345.33graphs/s]
Creating k-hop graphs:  56%|█████▌    | 6718/12000 [00:19<00:16, 329.92graphs/s]
Creating k-hop graphs:  56%|█████▋    | 6754/12000 [00:19<00:15, 336.58graphs/s]
Creating k-hop graphs:  57%|█████▋    | 6789/12000 [00:19<00:15, 338.60graphs/s]
Creating k-hop graphs:  57%|█████▋    | 6823/12000 [00:19<00:15, 335.82graphs/s]
Creating k-hop graphs:  57%|█████▋    | 6860/12000 [00:19<00:14, 345.65graphs/s]
Creating k-hop graphs:  57%|█████▋    | 6895/12000 [00:19<00:14, 345.87graphs/s]
Creating k-hop graphs:  58%|█████▊    | 6930/12000 [00:19<00:14, 345.11graphs/s]
Creating k-hop graphs:  58%|█████▊    | 6965/12000 [00:19<00:14, 342.36graphs/s]
Creating k-hop graphs:  58%|█████▊    | 7003/12000 [00:20<00:14, 351.22graphs/s]
Creating k-hop graphs:  59%|█████▊    | 7044/12000 [00:20<00:13, 366.74graphs/s]
Creating k-hop graphs:  59%|█████▉    | 7081/12000 [00:20<00:13, 365.01graphs/s]
Creating k-hop graphs:  59%|█████▉    | 7121/12000 [00:20<00:13, 375.15graphs/s]
Creating k-hop graphs:  60%|█████▉    | 7159/12000 [00:20<00:13, 357.97graphs/s]
Creating k-hop graphs:  60%|█████▉    | 7199/12000 [00:20<00:13, 365.73graphs/s]
Creating k-hop graphs:  60%|██████    | 7236/12000 [00:20<00:13, 366.31graphs/s]
Creating k-hop graphs:  61%|██████    | 7273/12000 [00:20<00:13, 355.00graphs/s]
Creating k-hop graphs:  61%|██████    | 7311/12000 [00:20<00:13, 360.66graphs/s]
Creating k-hop graphs:  61%|██████    | 7348/12000 [00:20<00:13, 351.07graphs/s]
Creating k-hop graphs:  62%|██████▏   | 7384/12000 [00:21<00:13, 353.44graphs/s]
Creating k-hop graphs:  62%|██████▏   | 7420/12000 [00:21<00:12, 352.44graphs/s]
Creating k-hop graphs:  62%|██████▏   | 7457/12000 [00:21<00:12, 351.28graphs/s]
Creating k-hop graphs:  62%|██████▏   | 7493/12000 [00:21<00:13, 345.71graphs/s]
Creating k-hop graphs:  63%|██████▎   | 7529/12000 [00:21<00:12, 348.63graphs/s]
Creating k-hop graphs:  63%|██████▎   | 7566/12000 [00:21<00:12, 350.21graphs/s]
Creating k-hop graphs:  63%|██████▎   | 7602/12000 [00:21<00:12, 342.43graphs/s]
Creating k-hop graphs:  64%|██████▎   | 7643/12000 [00:21<00:12, 360.73graphs/s]
Creating k-hop graphs:  64%|██████▍   | 7683/12000 [00:21<00:11, 370.36graphs/s]
Creating k-hop graphs:  64%|██████▍   | 7724/12000 [00:22<00:11, 380.49graphs/s]
Creating k-hop graphs:  65%|██████▍   | 7763/12000 [00:22<00:11, 370.38graphs/s]
Creating k-hop graphs:  65%|██████▌   | 7801/12000 [00:22<00:11, 363.43graphs/s]
Creating k-hop graphs:  65%|██████▌   | 7838/12000 [00:22<00:11, 360.66graphs/s]
Creating k-hop graphs:  66%|██████▌   | 7875/12000 [00:22<00:11, 354.11graphs/s]
Creating k-hop graphs:  66%|██████▌   | 7911/12000 [00:22<00:11, 346.95graphs/s]
Creating k-hop graphs:  66%|██████▌   | 7948/12000 [00:22<00:11, 351.84graphs/s]
Creating k-hop graphs:  67%|██████▋   | 7984/12000 [00:22<00:11, 349.12graphs/s]
Creating k-hop graphs:  67%|██████▋   | 8021/12000 [00:22<00:11, 352.49graphs/s]
Creating k-hop graphs:  67%|██████▋   | 8057/12000 [00:22<00:11, 352.74graphs/s]
Creating k-hop graphs:  67%|██████▋   | 8093/12000 [00:23<00:11, 351.59graphs/s]
Creating k-hop graphs:  68%|██████▊   | 8129/12000 [00:23<00:11, 347.61graphs/s]
Creating k-hop graphs:  68%|██████▊   | 8164/12000 [00:23<00:11, 340.99graphs/s]
Creating k-hop graphs:  68%|██████▊   | 8203/12000 [00:23<00:10, 352.13graphs/s]
Creating k-hop graphs:  69%|██████▊   | 8239/12000 [00:23<00:10, 349.12graphs/s]
Creating k-hop graphs:  69%|██████▉   | 8277/12000 [00:23<00:10, 357.28graphs/s]
Creating k-hop graphs:  69%|██████▉   | 8313/12000 [00:23<00:10, 355.07graphs/s]
Creating k-hop graphs:  70%|██████▉   | 8349/12000 [00:23<00:10, 340.53graphs/s]
Creating k-hop graphs:  70%|██████▉   | 8384/12000 [00:23<00:10, 339.23graphs/s]
Creating k-hop graphs:  70%|███████   | 8419/12000 [00:24<00:10, 335.64graphs/s]
Creating k-hop graphs:  70%|███████   | 8457/12000 [00:24<00:10, 346.97graphs/s]
Creating k-hop graphs:  71%|███████   | 8492/12000 [00:24<00:10, 347.00graphs/s]
Creating k-hop graphs:  71%|███████   | 8531/12000 [00:24<00:09, 358.73graphs/s]
Creating k-hop graphs:  71%|███████▏  | 8567/12000 [00:24<00:09, 351.72graphs/s]
Creating k-hop graphs:  72%|███████▏  | 8605/12000 [00:24<00:09, 356.59graphs/s]
Creating k-hop graphs:  72%|███████▏  | 8642/12000 [00:24<00:09, 357.05graphs/s]
Creating k-hop graphs:  72%|███████▏  | 8680/12000 [00:24<00:09, 361.84graphs/s]
Creating k-hop graphs:  73%|███████▎  | 8717/12000 [00:24<00:09, 362.70graphs/s]
Creating k-hop graphs:  73%|███████▎  | 8757/12000 [00:24<00:08, 373.25graphs/s]
Creating k-hop graphs:  73%|███████▎  | 8795/12000 [00:25<00:09, 349.98graphs/s]
Creating k-hop graphs:  74%|███████▎  | 8831/12000 [00:25<00:09, 331.01graphs/s]
Creating k-hop graphs:  74%|███████▍  | 8869/12000 [00:25<00:09, 343.06graphs/s]
Creating k-hop graphs:  74%|███████▍  | 8904/12000 [00:25<00:09, 328.97graphs/s]
Creating k-hop graphs:  75%|███████▍  | 8942/12000 [00:25<00:08, 340.23graphs/s]
Creating k-hop graphs:  75%|███████▍  | 8983/12000 [00:25<00:08, 358.79graphs/s]
Creating k-hop graphs:  75%|███████▌  | 9020/12000 [00:25<00:08, 351.41graphs/s]
Creating k-hop graphs:  75%|███████▌  | 9056/12000 [00:25<00:08, 341.54graphs/s]
Creating k-hop graphs:  76%|███████▌  | 9094/12000 [00:25<00:08, 347.67graphs/s]
Creating k-hop graphs:  76%|███████▌  | 9132/12000 [00:26<00:08, 356.54graphs/s]
Creating k-hop graphs:  76%|███████▋  | 9168/12000 [00:26<00:08, 349.22graphs/s]
Creating k-hop graphs:  77%|███████▋  | 9204/12000 [00:26<00:08, 345.37graphs/s]
Creating k-hop graphs:  77%|███████▋  | 9241/12000 [00:26<00:07, 351.46graphs/s]
Creating k-hop graphs:  77%|███████▋  | 9277/12000 [00:26<00:07, 351.28graphs/s]
Creating k-hop graphs:  78%|███████▊  | 9318/12000 [00:26<00:07, 362.52graphs/s]
Creating k-hop graphs:  78%|███████▊  | 9355/12000 [00:26<00:07, 356.41graphs/s]
Creating k-hop graphs:  78%|███████▊  | 9392/12000 [00:26<00:07, 356.18graphs/s]
Creating k-hop graphs:  79%|███████▊  | 9428/12000 [00:26<00:07, 353.44graphs/s]
Creating k-hop graphs:  79%|███████▉  | 9468/12000 [00:26<00:06, 366.70graphs/s]
Creating k-hop graphs:  79%|███████▉  | 9505/12000 [00:27<00:06, 357.89graphs/s]
Creating k-hop graphs:  80%|███████▉  | 9541/12000 [00:27<00:07, 350.39graphs/s]
Creating k-hop graphs:  80%|███████▉  | 9581/12000 [00:27<00:06, 362.92graphs/s]
Creating k-hop graphs:  80%|████████  | 9618/12000 [00:27<00:06, 364.82graphs/s]
Creating k-hop graphs:  80%|████████  | 9656/12000 [00:27<00:06, 368.44graphs/s]
Creating k-hop graphs:  81%|████████  | 9693/12000 [00:27<00:06, 355.63graphs/s]
Creating k-hop graphs:  81%|████████  | 9729/12000 [00:27<00:06, 350.16graphs/s]
Creating k-hop graphs:  81%|████████▏ | 9766/12000 [00:27<00:06, 354.00graphs/s]
Creating k-hop graphs:  82%|████████▏ | 9803/12000 [00:27<00:06, 358.03graphs/s]
Creating k-hop graphs:  82%|████████▏ | 9839/12000 [00:28<00:06, 348.05graphs/s]
Creating k-hop graphs:  82%|████████▏ | 9877/12000 [00:28<00:05, 355.52graphs/s]
Creating k-hop graphs:  83%|████████▎ | 9913/12000 [00:28<00:06, 345.79graphs/s]
Creating k-hop graphs:  83%|████████▎ | 9948/12000 [00:28<00:05, 343.03graphs/s]
Creating k-hop graphs:  83%|████████▎ | 9984/12000 [00:28<00:05, 347.37graphs/s]
Creating k-hop graphs:  83%|████████▎ | 10019/12000 [00:28<00:05, 343.55graphs/s]
Creating k-hop graphs:  84%|████████▍ | 10055/12000 [00:28<00:05, 346.27graphs/s]
Creating k-hop graphs:  84%|████████▍ | 10090/12000 [00:28<00:05, 342.21graphs/s]
Creating k-hop graphs:  84%|████████▍ | 10127/12000 [00:28<00:05, 349.75graphs/s]
Creating k-hop graphs:  85%|████████▍ | 10168/12000 [00:28<00:05, 365.78graphs/s]
Creating k-hop graphs:  85%|████████▌ | 10205/12000 [00:29<00:05, 349.59graphs/s]
Creating k-hop graphs:  85%|████████▌ | 10245/12000 [00:29<00:04, 362.67graphs/s]
Creating k-hop graphs:  86%|████████▌ | 10282/12000 [00:29<00:04, 356.61graphs/s]
Creating k-hop graphs:  86%|████████▌ | 10323/12000 [00:29<00:04, 370.93graphs/s]
Creating k-hop graphs:  86%|████████▋ | 10361/12000 [00:29<00:04, 366.12graphs/s]
Creating k-hop graphs:  87%|████████▋ | 10398/12000 [00:29<00:04, 365.46graphs/s]
Creating k-hop graphs:  87%|████████▋ | 10435/12000 [00:29<00:04, 350.92graphs/s]
Creating k-hop graphs:  87%|████████▋ | 10471/12000 [00:29<00:04, 343.41graphs/s]
Creating k-hop graphs:  88%|████████▊ | 10507/12000 [00:29<00:04, 345.69graphs/s]
Creating k-hop graphs:  88%|████████▊ | 10542/12000 [00:30<00:04, 344.20graphs/s]
Creating k-hop graphs:  88%|████████▊ | 10577/12000 [00:30<00:04, 329.98graphs/s]
Creating k-hop graphs:  88%|████████▊ | 10614/12000 [00:30<00:04, 339.36graphs/s]
Creating k-hop graphs:  89%|████████▊ | 10649/12000 [00:30<00:03, 341.61graphs/s]
Creating k-hop graphs:  89%|████████▉ | 10684/12000 [00:30<00:03, 343.26graphs/s]
Creating k-hop graphs:  89%|████████▉ | 10722/12000 [00:30<00:03, 353.59graphs/s]
Creating k-hop graphs:  90%|████████▉ | 10759/12000 [00:30<00:03, 357.92graphs/s]
Creating k-hop graphs:  90%|████████▉ | 10795/12000 [00:30<00:03, 355.21graphs/s]
Creating k-hop graphs:  90%|█████████ | 10831/12000 [00:30<00:03, 356.21graphs/s]
Creating k-hop graphs:  91%|█████████ | 10867/12000 [00:30<00:03, 354.18graphs/s]
Creating k-hop graphs:  91%|█████████ | 10906/12000 [00:31<00:03, 362.25graphs/s]
Creating k-hop graphs:  91%|█████████ | 10943/12000 [00:31<00:02, 359.64graphs/s]
Creating k-hop graphs:  91%|█████████▏| 10979/12000 [00:31<00:02, 354.42graphs/s]
Creating k-hop graphs:  92%|█████████▏| 11015/12000 [00:31<00:02, 355.68graphs/s]
Creating k-hop graphs:  92%|█████████▏| 11051/12000 [00:31<00:02, 344.19graphs/s]
Creating k-hop graphs:  92%|█████████▏| 11093/12000 [00:31<00:02, 363.25graphs/s]
Creating k-hop graphs:  93%|█████████▎| 11133/12000 [00:31<00:02, 372.90graphs/s]
Creating k-hop graphs:  93%|█████████▎| 11171/12000 [00:31<00:02, 353.71graphs/s]
Creating k-hop graphs:  93%|█████████▎| 11212/12000 [00:31<00:02, 366.80graphs/s]
Creating k-hop graphs:  94%|█████████▎| 11249/12000 [00:32<00:02, 356.23graphs/s]
Creating k-hop graphs:  94%|█████████▍| 11285/12000 [00:32<00:02, 350.08graphs/s]
Creating k-hop graphs:  94%|█████████▍| 11323/12000 [00:32<00:01, 355.98graphs/s]
Creating k-hop graphs:  95%|█████████▍| 11359/12000 [00:32<00:01, 356.68graphs/s]
Creating k-hop graphs:  95%|█████████▍| 11398/12000 [00:32<00:01, 365.37graphs/s]
Creating k-hop graphs:  95%|█████████▌| 11435/12000 [00:32<00:01, 359.74graphs/s]
Creating k-hop graphs:  96%|█████████▌| 11472/12000 [00:32<00:01, 349.17graphs/s]
Creating k-hop graphs:  96%|█████████▌| 11508/12000 [00:32<00:01, 349.15graphs/s]
Creating k-hop graphs:  96%|█████████▌| 11543/12000 [00:32<00:01, 299.58graphs/s]
Creating k-hop graphs:  96%|█████████▋| 11580/12000 [00:33<00:01, 315.06graphs/s]
Creating k-hop graphs:  97%|█████████▋| 11616/12000 [00:33<00:01, 326.69graphs/s]
Creating k-hop graphs:  97%|█████████▋| 11652/12000 [00:33<00:01, 332.77graphs/s]
Creating k-hop graphs:  97%|█████████▋| 11691/12000 [00:33<00:00, 346.82graphs/s]
Creating k-hop graphs:  98%|█████████▊| 11730/12000 [00:33<00:00, 355.35graphs/s]
Creating k-hop graphs:  98%|█████████▊| 11766/12000 [00:33<00:00, 344.39graphs/s]
Creating k-hop graphs:  98%|█████████▊| 11802/12000 [00:33<00:00, 346.84graphs/s]
Creating k-hop graphs:  99%|█████████▊| 11837/12000 [00:33<00:00, 339.67graphs/s]
Creating k-hop graphs:  99%|█████████▉| 11872/12000 [00:33<00:00, 338.60graphs/s]
Creating k-hop graphs:  99%|█████████▉| 11908/12000 [00:33<00:00, 342.66graphs/s]
Creating k-hop graphs: 100%|█████████▉| 11943/12000 [00:34<00:00, 339.09graphs/s]
Creating k-hop graphs: 100%|█████████▉| 11977/12000 [00:34<00:00, 331.34graphs/s]
Creating k-hop graphs: 100%|██████████| 12000/12000 [00:34<00:00, 349.68graphs/s]
Starting experiment
task: Task.ZINC
type: GNN_TYPE.GCN
dim: 32
depth: 3
num_layers: 3
train_fraction: 0.8
max_epochs: 50000
eval_every: 10
batch_size: 1024
accum_grad: 1
patience: 20
stop: STOP.TRAIN
loader_workers: 0
last_layer: LAST_LAYER.K_HOP
k_hop: 5
no_layer_norm: False
no_activation: False
no_residual: False
unroll: False
max_samples: 32000
learning_rate: 0.001
weight_decay: 0.0

Training examples: 9600, test examples: 2400
Starting training
Epoch 10, LR: [0.001]: Train loss: 2.9821942, Test loss: 3.9427964 (new best train)
Epoch 20, LR: [0.001]: Train loss: 2.5636572, Test loss: 3.5606553 (new best train)
Epoch 30, LR: [0.001]: Train loss: 2.1649305, Test loss: 3.0240539 (new best train)
Epoch 40, LR: [0.001]: Train loss: 1.8398693, Test loss: 2.7323221 (new best train)
Epoch 50, LR: [0.001]: Train loss: 1.6770957, Test loss: 2.6244360 (new best train)
Epoch 60, LR: [0.001]: Train loss: 1.5724272, Test loss: 2.5763575 (new best train)
Epoch 70, LR: [0.001]: Train loss: 1.5521613, Test loss: 2.5206388 (new best train)
Epoch 80, LR: [0.001]: Train loss: 1.5107903, Test loss: 2.5049366 (new best train)
Epoch 90, LR: [0.001]: Train loss: 1.5021196, Test loss: 2.4715648 (new best train)
Epoch 100, LR: [0.001]: Train loss: 1.4841992, Test loss: 2.4626120 (new best train)
Epoch 110, LR: [0.001]: Train loss: 1.4708037, Test loss: 2.5606451 (new best train)
Epoch 120, LR: [0.001]: Train loss: 1.4428683, Test loss: 2.4573590 (new best train)
Epoch 130, LR: [0.001]: Train loss: 1.4263419, Test loss: 2.4111935 (new best train)
Epoch 140, LR: [0.001]: Train loss: 1.4408155, Test loss: 2.4641845
Epoch 150, LR: [0.001]: Train loss: 1.4097238, Test loss: 2.3935728 (new best train)
Epoch 160, LR: [0.001]: Train loss: 1.4038736, Test loss: 2.4212935 (new best train)
Epoch 170, LR: [0.001]: Train loss: 1.3859961, Test loss: 2.3819238 (new best train)
Epoch 180, LR: [0.001]: Train loss: 1.3878220, Test loss: 2.3757965
Epoch 190, LR: [0.001]: Train loss: 1.3674981, Test loss: 2.3384798 (new best train)
Epoch 200, LR: [0.001]: Train loss: 1.3574010, Test loss: 2.3442484 (new best train)
Epoch 210, LR: [0.001]: Train loss: 1.3815609, Test loss: 2.3583662
Epoch 220, LR: [0.001]: Train loss: 1.3616212, Test loss: 2.3334044
Epoch 230, LR: [0.001]: Train loss: 1.3597892, Test loss: 2.3101186
Epoch 240, LR: [0.001]: Train loss: 1.3338112, Test loss: 2.3351085 (new best train)
Epoch 250, LR: [0.001]: Train loss: 1.3375088, Test loss: 2.3109391
Epoch 260, LR: [0.001]: Train loss: 1.3274982, Test loss: 2.2764065 (new best train)
Epoch 270, LR: [0.001]: Train loss: 1.3299200, Test loss: 2.2936023
Epoch 280, LR: [0.001]: Train loss: 1.3101289, Test loss: 2.2670647 (new best train)
Epoch 290, LR: [0.001]: Train loss: 1.3058551, Test loss: 2.2601100 (new best train)
Epoch 300, LR: [0.001]: Train loss: 1.2926847, Test loss: 2.2518333 (new best train)
Epoch 310, LR: [0.001]: Train loss: 1.3053250, Test loss: 2.2494446
Epoch 320, LR: [0.001]: Train loss: 1.2981113, Test loss: 2.2718128
Epoch 330, LR: [0.001]: Train loss: 1.2751122, Test loss: 2.2542720 (new best train)
Epoch 340, LR: [0.001]: Train loss: 1.2703332, Test loss: 2.2270560 (new best train)
Epoch 350, LR: [0.001]: Train loss: 1.2830738, Test loss: 2.2329093
Epoch 360, LR: [0.001]: Train loss: 1.2618269, Test loss: 2.2718725 (new best train)
Epoch 370, LR: [0.001]: Train loss: 1.2719390, Test loss: 2.2365318
Epoch 380, LR: [0.001]: Train loss: 1.2504325, Test loss: 2.2848063 (new best train)
Epoch 390, LR: [0.001]: Train loss: 1.2719917, Test loss: 2.2426208
Epoch 400, LR: [0.001]: Train loss: 1.2574465, Test loss: 2.2007395
Epoch 410, LR: [0.001]: Train loss: 1.2629660, Test loss: 2.2153672
Epoch 420, LR: [0.001]: Train loss: 1.2521567, Test loss: 2.2067371
Epoch 430, LR: [0.001]: Train loss: 1.2412592, Test loss: 2.2017078 (new best train)
Epoch 440, LR: [0.001]: Train loss: 1.2243481, Test loss: 2.1910771 (new best train)
Epoch 450, LR: [0.001]: Train loss: 1.2342376, Test loss: 2.2631663
Epoch 460, LR: [0.001]: Train loss: 1.2438997, Test loss: 2.1860122
Epoch 470, LR: [0.001]: Train loss: 1.2189602, Test loss: 2.2566257 (new best train)
Epoch 480, LR: [0.001]: Train loss: 1.2374327, Test loss: 2.1952608
Epoch 490, LR: [0.001]: Train loss: 1.2168234, Test loss: 2.2004171 (new best train)
Epoch 500, LR: [0.001]: Train loss: 1.2035335, Test loss: 2.1626108 (new best train)
Epoch 510, LR: [0.001]: Train loss: 1.2040688, Test loss: 2.1876362
Epoch 520, LR: [0.001]: Train loss: 1.2053694, Test loss: 2.1517713
Epoch 530, LR: [0.001]: Train loss: 1.2014544, Test loss: 2.1447706 (new best train)
Epoch 540, LR: [0.001]: Train loss: 1.2023256, Test loss: 2.2441862
Epoch 550, LR: [0.001]: Train loss: 1.2087812, Test loss: 2.1510756
Epoch 560, LR: [0.001]: Train loss: 1.1934670, Test loss: 2.1596142 (new best train)
Epoch 570, LR: [0.001]: Train loss: 1.1857998, Test loss: 2.1874394 (new best train)
Epoch 580, LR: [0.001]: Train loss: 1.1813575, Test loss: 2.1305204 (new best train)
Epoch 590, LR: [0.001]: Train loss: 1.1853698, Test loss: 2.1994119
Epoch 600, LR: [0.001]: Train loss: 1.1895390, Test loss: 2.1670761
Epoch 610, LR: [0.001]: Train loss: 1.1895858, Test loss: 2.1538700
Epoch 620, LR: [0.001]: Train loss: 1.1719513, Test loss: 2.1728656 (new best train)
Epoch 630, LR: [0.001]: Train loss: 1.1644047, Test loss: 2.1153195 (new best train)
Epoch 640, LR: [0.001]: Train loss: 1.1699787, Test loss: 2.1787717
Epoch 650, LR: [0.001]: Train loss: 1.1620060, Test loss: 2.1135214 (new best train)
Epoch 660, LR: [0.001]: Train loss: 1.1541760, Test loss: 2.1195640 (new best train)
Epoch 670, LR: [0.001]: Train loss: 1.1681690, Test loss: 2.1850047
Epoch 680, LR: [0.001]: Train loss: 1.1813890, Test loss: 2.1031006
Epoch 690, LR: [0.001]: Train loss: 1.1580539, Test loss: 2.1502073
Epoch 700, LR: [0.001]: Train loss: 1.1563705, Test loss: 2.0939775
Epoch 710, LR: [0.001]: Train loss: 1.1435983, Test loss: 2.1006297 (new best train)
Epoch 720, LR: [0.001]: Train loss: 1.1450058, Test loss: 2.1090466
Epoch 730, LR: [0.001]: Train loss: 1.1452897, Test loss: 2.0980439
Epoch 740, LR: [0.001]: Train loss: 1.1579458, Test loss: 2.0943481
Epoch 750, LR: [0.001]: Train loss: 1.1591390, Test loss: 2.1162088
Epoch 760, LR: [0.001]: Train loss: 1.1364274, Test loss: 2.0775532 (new best train)
Epoch 770, LR: [0.001]: Train loss: 1.1460007, Test loss: 2.1110678
Epoch 780, LR: [0.001]: Train loss: 1.1353365, Test loss: 2.1000820 (new best train)
Epoch 790, LR: [0.001]: Train loss: 1.1267444, Test loss: 2.0880607 (new best train)
Epoch 800, LR: [0.001]: Train loss: 1.1360360, Test loss: 2.1018030
Epoch 810, LR: [0.001]: Train loss: 1.1284297, Test loss: 2.1514119
Epoch 820, LR: [0.001]: Train loss: 1.1416782, Test loss: 2.0831985
Epoch 830, LR: [0.001]: Train loss: 1.1199762, Test loss: 2.0708893 (new best train)
Epoch 840, LR: [0.001]: Train loss: 1.1101118, Test loss: 2.0823151 (new best train)
Epoch 850, LR: [0.001]: Train loss: 1.1289247, Test loss: 2.0900222
Epoch 860, LR: [0.001]: Train loss: 1.1169461, Test loss: 2.0896040
Epoch 870, LR: [0.001]: Train loss: 1.1093429, Test loss: 2.1887183 (new best train)
Epoch 880, LR: [0.001]: Train loss: 1.1324792, Test loss: 2.0809188
Epoch 890, LR: [0.001]: Train loss: 1.1206710, Test loss: 2.0773018
Epoch 900, LR: [0.001]: Train loss: 1.1037759, Test loss: 2.0686065 (new best train)
Epoch 910, LR: [0.001]: Train loss: 1.1214749, Test loss: 2.0625580
Epoch 920, LR: [0.001]: Train loss: 1.1061633, Test loss: 2.0479063
Epoch 930, LR: [0.001]: Train loss: 1.0999558, Test loss: 2.0582902 (new best train)
Epoch 940, LR: [0.001]: Train loss: 1.1042664, Test loss: 2.0673008
Epoch 950, LR: [0.001]: Train loss: 1.1331116, Test loss: 2.1679127
Epoch 960, LR: [0.001]: Train loss: 1.0996247, Test loss: 2.0788021 (new best train)
Epoch 970, LR: [0.001]: Train loss: 1.1025127, Test loss: 2.1260418
Epoch 980, LR: [0.001]: Train loss: 1.0944280, Test loss: 2.0604428 (new best train)
Epoch 990, LR: [0.001]: Train loss: 1.1018543, Test loss: 2.0410784
Epoch 1000, LR: [0.001]: Train loss: 1.1115926, Test loss: 2.0504636
Epoch 1010, LR: [0.001]: Train loss: 1.0948083, Test loss: 2.0927853
Epoch 1020, LR: [0.001]: Train loss: 1.1024112, Test loss: 2.0665281
Epoch 1030, LR: [0.001]: Train loss: 1.0900986, Test loss: 2.0830640 (new best train)
Epoch 1040, LR: [0.001]: Train loss: 1.0892672, Test loss: 2.0624536 (new best train)
Epoch 1050, LR: [0.001]: Train loss: 1.0851565, Test loss: 2.0501169 (new best train)
Epoch 1060, LR: [0.001]: Train loss: 1.0912323, Test loss: 2.0892501
Epoch 1070, LR: [0.001]: Train loss: 1.0973250, Test loss: 2.0643518
Epoch 1080, LR: [0.001]: Train loss: 1.0895519, Test loss: 2.0404755
Epoch 1090, LR: [0.001]: Train loss: 1.1043858, Test loss: 2.0692199
Epoch 1100, LR: [0.001]: Train loss: 1.0799433, Test loss: 2.1263983 (new best train)
Epoch 1110, LR: [0.001]: Train loss: 1.1010511, Test loss: 2.1471128
Epoch 1120, LR: [0.001]: Train loss: 1.0909562, Test loss: 2.0360627
Epoch 1130, LR: [0.001]: Train loss: 1.0806255, Test loss: 2.0885907
Epoch 1140, LR: [0.001]: Train loss: 1.0791305, Test loss: 2.1040774 (new best train)
Epoch 1150, LR: [0.001]: Train loss: 1.0948015, Test loss: 2.0301178
Epoch 1160, LR: [0.001]: Train loss: 1.0782832, Test loss: 2.0435467 (new best train)
Epoch 1170, LR: [0.001]: Train loss: 1.0832069, Test loss: 2.0326403
Epoch 1180, LR: [0.001]: Train loss: 1.0826358, Test loss: 2.0798935
Epoch 1190, LR: [0.001]: Train loss: 1.0845484, Test loss: 2.0151272
Epoch 1200, LR: [0.001]: Train loss: 1.0771848, Test loss: 2.0921981 (new best train)
Epoch 1210, LR: [0.001]: Train loss: 1.0870287, Test loss: 2.0278037
Epoch 1220, LR: [0.001]: Train loss: 1.0648925, Test loss: 2.1028659 (new best train)
Epoch 1230, LR: [0.001]: Train loss: 1.0754171, Test loss: 2.0408123
Epoch 1240, LR: [0.001]: Train loss: 1.0643516, Test loss: 2.0209623 (new best train)
Epoch 1250, LR: [0.001]: Train loss: 1.0900734, Test loss: 2.0448778
Epoch 1260, LR: [0.001]: Train loss: 1.0790259, Test loss: 2.0538809
Epoch 1270, LR: [0.001]: Train loss: 1.0815777, Test loss: 2.0506147
Epoch 1280, LR: [0.001]: Train loss: 1.0756060, Test loss: 2.0376953
Epoch 1290, LR: [0.001]: Train loss: 1.0847035, Test loss: 2.1175549
Epoch 1300, LR: [0.001]: Train loss: 1.0649608, Test loss: 2.0320449
Epoch 1310, LR: [0.001]: Train loss: 1.0568120, Test loss: 2.0385692 (new best train)
Epoch 1320, LR: [0.001]: Train loss: 1.0558668, Test loss: 2.0589379 (new best train)
Epoch 1330, LR: [0.001]: Train loss: 1.0648671, Test loss: 2.1098212
Epoch 1340, LR: [0.001]: Train loss: 1.0588790, Test loss: 2.0429254
Epoch 1350, LR: [0.001]: Train loss: 1.0848016, Test loss: 2.0597680
Epoch 1360, LR: [0.001]: Train loss: 1.0706400, Test loss: 2.0678850
Epoch 1370, LR: [0.001]: Train loss: 1.0665435, Test loss: 2.0599306
Epoch 1380, LR: [0.001]: Train loss: 1.0689797, Test loss: 2.1334396
Epoch 1390, LR: [0.001]: Train loss: 1.0598028, Test loss: 2.0247103
Epoch 1400, LR: [0.001]: Train loss: 1.0629222, Test loss: 2.1084621
Epoch 1410, LR: [0.001]: Train loss: 1.0611045, Test loss: 2.1124209
Epoch 1420, LR: [0.001]: Train loss: 1.0730610, Test loss: 2.1024906
Epoch 1430, LR: [0.001]: Train loss: 1.0502985, Test loss: 2.0655496 (new best train)
Epoch 1440, LR: [0.001]: Train loss: 1.0471257, Test loss: 2.0383748 (new best train)
Epoch 1450, LR: [0.001]: Train loss: 1.0500891, Test loss: 2.0329869
Epoch 1460, LR: [0.001]: Train loss: 1.0506677, Test loss: 2.0544180
Epoch 1470, LR: [0.001]: Train loss: 1.0719117, Test loss: 2.0391636
Epoch 1480, LR: [0.001]: Train loss: 1.0479202, Test loss: 2.0215681
Epoch 1490, LR: [0.001]: Train loss: 1.0454198, Test loss: 2.0176344 (new best train)
Epoch 1500, LR: [0.001]: Train loss: 1.0420469, Test loss: 2.0336809 (new best train)
Epoch 1510, LR: [0.001]: Train loss: 1.0467458, Test loss: 2.0266331
Epoch 1520, LR: [0.001]: Train loss: 1.0378194, Test loss: 2.0368101 (new best train)
Epoch 1530, LR: [0.001]: Train loss: 1.0440888, Test loss: 2.0231605
Epoch 1540, LR: [0.001]: Train loss: 1.0523750, Test loss: 2.0939142
Epoch 1550, LR: [0.001]: Train loss: 1.0583815, Test loss: 2.0197099
Epoch 1560, LR: [0.001]: Train loss: 1.0374511, Test loss: 2.0469467 (new best train)
Epoch 1570, LR: [0.001]: Train loss: 1.0446332, Test loss: 2.0086959
Epoch 1580, LR: [0.001]: Train loss: 1.0610575, Test loss: 2.1322175
Epoch 1590, LR: [0.001]: Train loss: 1.0428814, Test loss: 2.0289909
Epoch 1600, LR: [0.001]: Train loss: 1.0572731, Test loss: 2.0235744
Epoch 1610, LR: [0.001]: Train loss: 1.0522238, Test loss: 2.0467643
Epoch 1620, LR: [0.001]: Train loss: 1.0417626, Test loss: 2.0473501
Epoch 1630, LR: [0.001]: Train loss: 1.0411304, Test loss: 2.0125813
Epoch 1640, LR: [0.001]: Train loss: 1.0416194, Test loss: 2.0444291
Epoch 1650, LR: [0.001]: Train loss: 1.0490349, Test loss: 2.0280997
Epoch 1660, LR: [0.001]: Train loss: 1.0468805, Test loss: 2.0446900
Epoch 1670, LR: [0.001]: Train loss: 1.0323939, Test loss: 2.0325187 (new best train)
Epoch 1680, LR: [0.001]: Train loss: 1.0319026, Test loss: 2.0451604 (new best train)
Epoch 1690, LR: [0.001]: Train loss: 1.0383645, Test loss: 2.1506918
Epoch 1700, LR: [0.001]: Train loss: 1.0612346, Test loss: 2.0780317
Epoch 1710, LR: [0.001]: Train loss: 1.0277574, Test loss: 2.0034698 (new best train)
Epoch 1720, LR: [0.001]: Train loss: 1.0508238, Test loss: 2.0410187
Epoch 1730, LR: [0.001]: Train loss: 1.0257281, Test loss: 2.0258377 (new best train)
Epoch 1740, LR: [0.001]: Train loss: 1.0330814, Test loss: 2.0590791
Epoch 1750, LR: [0.001]: Train loss: 1.0297657, Test loss: 2.0302790
Epoch 1760, LR: [0.001]: Train loss: 1.0429509, Test loss: 1.9968717
Epoch 1770, LR: [0.001]: Train loss: 1.0301697, Test loss: 2.0309403
Epoch 1780, LR: [0.001]: Train loss: 1.0308129, Test loss: 2.0240669
Epoch 1790, LR: [0.001]: Train loss: 1.0216164, Test loss: 2.0614433 (new best train)
Epoch 1800, LR: [0.001]: Train loss: 1.0330868, Test loss: 2.0076017
Epoch 1810, LR: [0.001]: Train loss: 1.0166238, Test loss: 2.0106204 (new best train)
Epoch 1820, LR: [0.001]: Train loss: 1.0318865, Test loss: 2.0565083
Epoch 1830, LR: [0.001]: Train loss: 1.0209658, Test loss: 2.1165638
Epoch 1840, LR: [0.001]: Train loss: 1.0539047, Test loss: 2.0184004
Epoch 1850, LR: [0.001]: Train loss: 1.0228117, Test loss: 2.0233422
Epoch 1860, LR: [0.001]: Train loss: 1.0193247, Test loss: 2.0628410
Epoch 1870, LR: [0.001]: Train loss: 1.0160371, Test loss: 2.0118093 (new best train)
Epoch 1880, LR: [0.001]: Train loss: 1.0246018, Test loss: 2.0171318
Epoch 1890, LR: [0.001]: Train loss: 1.0143909, Test loss: 2.0251563 (new best train)
Epoch 1900, LR: [0.001]: Train loss: 1.0240306, Test loss: 2.0208884
Epoch 1910, LR: [0.001]: Train loss: 1.0162956, Test loss: 2.0150140
Epoch 1920, LR: [0.001]: Train loss: 1.0202543, Test loss: 2.0146472
Epoch 1930, LR: [0.001]: Train loss: 1.0498196, Test loss: 2.0325190
Epoch 1940, LR: [0.001]: Train loss: 1.0181296, Test loss: 2.0202710
Epoch 1950, LR: [0.001]: Train loss: 1.0150965, Test loss: 2.0444344
Epoch 1960, LR: [0.001]: Train loss: 1.0312879, Test loss: 2.0252806
Epoch 1970, LR: [0.001]: Train loss: 1.0110947, Test loss: 2.0025239 (new best train)
Epoch 1980, LR: [0.001]: Train loss: 1.0202980, Test loss: 2.0016770
Epoch 1990, LR: [0.001]: Train loss: 1.0178501, Test loss: 1.9949767
Epoch 2000, LR: [0.001]: Train loss: 1.0180170, Test loss: 2.0193991
Epoch 2010, LR: [0.001]: Train loss: 1.0200375, Test loss: 2.0020106
Epoch 2020, LR: [0.001]: Train loss: 1.0081704, Test loss: 2.0189223 (new best train)
Epoch 2030, LR: [0.001]: Train loss: 1.0000851, Test loss: 2.0176122 (new best train)
Epoch 2040, LR: [0.001]: Train loss: 1.0231749, Test loss: 2.0218928
Epoch 2050, LR: [0.001]: Train loss: 1.0302785, Test loss: 2.1039304
Epoch 2060, LR: [0.001]: Train loss: 1.0159213, Test loss: 2.0749551
Epoch 2070, LR: [0.001]: Train loss: 1.0156962, Test loss: 2.0087909
Epoch 2080, LR: [0.001]: Train loss: 1.0055186, Test loss: 2.0611953
Epoch 2090, LR: [0.001]: Train loss: 1.0152412, Test loss: 2.0110706
Epoch 2100, LR: [0.001]: Train loss: 1.0163968, Test loss: 2.0270183
Epoch 2110, LR: [0.001]: Train loss: 0.9998378, Test loss: 2.0018050 (new best train)
Epoch 2120, LR: [0.001]: Train loss: 1.0016522, Test loss: 2.0202508
Epoch 2130, LR: [0.001]: Train loss: 1.0207894, Test loss: 2.0104195
Epoch 2140, LR: [0.001]: Train loss: 0.9947169, Test loss: 1.9915692 (new best train)
Epoch 2150, LR: [0.001]: Train loss: 1.0086232, Test loss: 2.0402236
Epoch 2160, LR: [0.001]: Train loss: 1.0036900, Test loss: 2.0084701
Epoch 2170, LR: [0.001]: Train loss: 1.0153893, Test loss: 2.0147572
Epoch 2180, LR: [0.001]: Train loss: 1.0171743, Test loss: 1.9859830
Epoch 2190, LR: [0.001]: Train loss: 0.9963509, Test loss: 2.2065659
Epoch 2200, LR: [0.001]: Train loss: 1.0247580, Test loss: 2.0019544
Epoch 2210, LR: [0.001]: Train loss: 1.0027283, Test loss: 2.0182358
Epoch 2220, LR: [0.001]: Train loss: 1.0053165, Test loss: 2.0267983
Epoch 2230, LR: [0.001]: Train loss: 1.0118144, Test loss: 2.0515648
Epoch 2240, LR: [0.001]: Train loss: 0.9970472, Test loss: 1.9935931
Epoch 2250, LR: [0.0005]: Train loss: 0.9967837, Test loss: 2.0329424
Epoch 2260, LR: [0.0005]: Train loss: 0.9735542, Test loss: 1.9875281 (new best train)
Epoch 2270, LR: [0.0005]: Train loss: 0.9870103, Test loss: 2.0196380
Epoch 2280, LR: [0.0005]: Train loss: 0.9769883, Test loss: 2.0068940
Epoch 2290, LR: [0.0005]: Train loss: 0.9805208, Test loss: 2.0527181
Epoch 2300, LR: [0.0005]: Train loss: 0.9727596, Test loss: 2.0258971 (new best train)
Epoch 2310, LR: [0.0005]: Train loss: 0.9748140, Test loss: 1.9959238
Epoch 2320, LR: [0.0005]: Train loss: 0.9728734, Test loss: 2.0579200
Epoch 2330, LR: [0.0005]: Train loss: 0.9814777, Test loss: 2.0285569
Epoch 2340, LR: [0.0005]: Train loss: 0.9722923, Test loss: 1.9965967 (new best train)
Epoch 2350, LR: [0.0005]: Train loss: 0.9730290, Test loss: 1.9934366
Epoch 2360, LR: [0.0005]: Train loss: 0.9696843, Test loss: 1.9952813 (new best train)
Epoch 2370, LR: [0.0005]: Train loss: 0.9776222, Test loss: 2.0513351
Epoch 2380, LR: [0.0005]: Train loss: 0.9807919, Test loss: 1.9931235
Epoch 2390, LR: [0.0005]: Train loss: 0.9666777, Test loss: 2.0218357 (new best train)
Epoch 2400, LR: [0.0005]: Train loss: 0.9664116, Test loss: 1.9983458 (new best train)
Epoch 2410, LR: [0.0005]: Train loss: 0.9690750, Test loss: 1.9832191
Epoch 2420, LR: [0.0005]: Train loss: 0.9759674, Test loss: 2.0257845
Epoch 2430, LR: [0.0005]: Train loss: 0.9704605, Test loss: 1.9967042
Epoch 2440, LR: [0.0005]: Train loss: 0.9647419, Test loss: 2.0153074 (new best train)
Epoch 2450, LR: [0.0005]: Train loss: 0.9730365, Test loss: 1.9853695
Epoch 2460, LR: [0.0005]: Train loss: 0.9758223, Test loss: 2.0192738
Epoch 2470, LR: [0.0005]: Train loss: 0.9718620, Test loss: 2.0269518
Epoch 2480, LR: [0.0005]: Train loss: 0.9658142, Test loss: 1.9796962
Epoch 2490, LR: [0.0005]: Train loss: 0.9705121, Test loss: 1.9884553
Epoch 2500, LR: [0.0005]: Train loss: 0.9600198, Test loss: 1.9909634 (new best train)
Epoch 2510, LR: [0.0005]: Train loss: 0.9689878, Test loss: 2.0477795
Epoch 2520, LR: [0.0005]: Train loss: 0.9833703, Test loss: 1.9862651
Epoch 2530, LR: [0.0005]: Train loss: 0.9629611, Test loss: 2.0036313
Epoch 2540, LR: [0.0005]: Train loss: 0.9673817, Test loss: 2.0141879
Epoch 2550, LR: [0.0005]: Train loss: 0.9666388, Test loss: 2.0060490
Epoch 2560, LR: [0.0005]: Train loss: 0.9627192, Test loss: 1.9844583
Epoch 2570, LR: [0.0005]: Train loss: 0.9719935, Test loss: 1.9995500
Epoch 2580, LR: [0.0005]: Train loss: 0.9613853, Test loss: 2.0600757
Epoch 2590, LR: [0.0005]: Train loss: 0.9610705, Test loss: 1.9895693
Epoch 2600, LR: [0.0005]: Train loss: 0.9576378, Test loss: 2.0122971 (new best train)
Epoch 2610, LR: [0.0005]: Train loss: 0.9613628, Test loss: 1.9723908
Epoch 2620, LR: [0.0005]: Train loss: 0.9633459, Test loss: 1.9784757
Epoch 2630, LR: [0.0005]: Train loss: 0.9576530, Test loss: 1.9916731
Epoch 2640, LR: [0.0005]: Train loss: 0.9615313, Test loss: 2.0104800
Epoch 2650, LR: [0.0005]: Train loss: 0.9575948, Test loss: 1.9829237
Epoch 2660, LR: [0.0005]: Train loss: 0.9571209, Test loss: 1.9852133 (new best train)
Epoch 2670, LR: [0.0005]: Train loss: 0.9559368, Test loss: 1.9863631 (new best train)
Epoch 2680, LR: [0.0005]: Train loss: 0.9707031, Test loss: 2.0092675
Epoch 2690, LR: [0.0005]: Train loss: 0.9607980, Test loss: 2.0546696
Epoch 2700, LR: [0.0005]: Train loss: 0.9594674, Test loss: 1.9861837
Epoch 2710, LR: [0.0005]: Train loss: 0.9553422, Test loss: 2.0110111 (new best train)
Epoch 2720, LR: [0.0005]: Train loss: 0.9718401, Test loss: 2.0369971
Epoch 2730, LR: [0.0005]: Train loss: 0.9577277, Test loss: 1.9961934
Epoch 2740, LR: [0.0005]: Train loss: 0.9518139, Test loss: 1.9954669 (new best train)
Epoch 2750, LR: [0.0005]: Train loss: 0.9550266, Test loss: 1.9882512
Epoch 2760, LR: [0.0005]: Train loss: 0.9546639, Test loss: 2.0094283
Epoch 2770, LR: [0.0005]: Train loss: 0.9490218, Test loss: 2.0028305 (new best train)
Epoch 2780, LR: [0.0005]: Train loss: 0.9523623, Test loss: 1.9863285
Epoch 2790, LR: [0.0005]: Train loss: 0.9526364, Test loss: 1.9831091
Epoch 2800, LR: [0.0005]: Train loss: 0.9592402, Test loss: 1.9856016
Epoch 2810, LR: [0.0005]: Train loss: 0.9491000, Test loss: 1.9843358
Epoch 2820, LR: [0.0005]: Train loss: 0.9566932, Test loss: 2.0066203
Epoch 2830, LR: [0.0005]: Train loss: 0.9488323, Test loss: 1.9760457 (new best train)
Epoch 2840, LR: [0.0005]: Train loss: 0.9485918, Test loss: 1.9794503 (new best train)
Epoch 2850, LR: [0.0005]: Train loss: 0.9628146, Test loss: 2.0091181
Epoch 2860, LR: [0.0005]: Train loss: 0.9609734, Test loss: 1.9822394
Epoch 2870, LR: [0.0005]: Train loss: 0.9516529, Test loss: 1.9847940
Epoch 2880, LR: [0.0005]: Train loss: 0.9543365, Test loss: 2.0193678
Epoch 2890, LR: [0.0005]: Train loss: 0.9590127, Test loss: 2.0108110
Epoch 2900, LR: [0.0005]: Train loss: 0.9573198, Test loss: 2.0108868
Epoch 2910, LR: [0.0005]: Train loss: 0.9470207, Test loss: 2.0039342 (new best train)
Epoch 2920, LR: [0.0005]: Train loss: 0.9416780, Test loss: 1.9924037 (new best train)
Epoch 2930, LR: [0.0005]: Train loss: 0.9430025, Test loss: 2.0312300
Epoch 2940, LR: [0.0005]: Train loss: 0.9456336, Test loss: 1.9734714
Epoch 2950, LR: [0.0005]: Train loss: 0.9428633, Test loss: 1.9891319
Epoch 2960, LR: [0.0005]: Train loss: 0.9440123, Test loss: 2.0063255
Epoch 2970, LR: [0.0005]: Train loss: 0.9407558, Test loss: 1.9999886 (new best train)
Epoch 2980, LR: [0.0005]: Train loss: 0.9437430, Test loss: 2.0135682
Epoch 2990, LR: [0.0005]: Train loss: 0.9602888, Test loss: 1.9804541
Epoch 3000, LR: [0.0005]: Train loss: 0.9437003, Test loss: 1.9876788
Epoch 3010, LR: [0.0005]: Train loss: 0.9420423, Test loss: 1.9934475
Epoch 3020, LR: [0.0005]: Train loss: 0.9433834, Test loss: 1.9792873
Epoch 3030, LR: [0.0005]: Train loss: 0.9470552, Test loss: 2.0057306
Epoch 3040, LR: [0.0005]: Train loss: 0.9409710, Test loss: 1.9788058
Epoch 3050, LR: [0.0005]: Train loss: 0.9428949, Test loss: 1.9975969
Epoch 3060, LR: [0.0005]: Train loss: 0.9421241, Test loss: 2.0054886
Epoch 3070, LR: [0.0005]: Train loss: 0.9473898, Test loss: 1.9844118
Epoch 3080, LR: [0.0005]: Train loss: 0.9401328, Test loss: 1.9936715 (new best train)
Epoch 3090, LR: [0.0005]: Train loss: 0.9413302, Test loss: 2.0168786
Epoch 3100, LR: [0.0005]: Train loss: 0.9424838, Test loss: 1.9803189
Epoch 3110, LR: [0.0005]: Train loss: 0.9403950, Test loss: 2.0099289
Epoch 3120, LR: [0.0005]: Train loss: 0.9405299, Test loss: 2.0346222
Epoch 3130, LR: [0.0005]: Train loss: 0.9429908, Test loss: 1.9854532
Epoch 3140, LR: [0.0005]: Train loss: 0.9357088, Test loss: 2.0061121 (new best train)
Epoch 3150, LR: [0.0005]: Train loss: 0.9408466, Test loss: 2.0291658
Epoch 3160, LR: [0.0005]: Train loss: 0.9501637, Test loss: 2.0029678
Epoch 3170, LR: [0.0005]: Train loss: 0.9444467, Test loss: 1.9931071
Epoch 3180, LR: [0.0005]: Train loss: 0.9354562, Test loss: 1.9955657 (new best train)
Epoch 3190, LR: [0.0005]: Train loss: 0.9347935, Test loss: 1.9868284 (new best train)
Epoch 3200, LR: [0.0005]: Train loss: 0.9430193, Test loss: 2.0857589
Epoch 3210, LR: [0.0005]: Train loss: 0.9457528, Test loss: 2.0058853
Epoch 3220, LR: [0.0005]: Train loss: 0.9316438, Test loss: 1.9917488 (new best train)
Epoch 3230, LR: [0.0005]: Train loss: 0.9319717, Test loss: 1.9727306
Epoch 3240, LR: [0.0005]: Train loss: 0.9304388, Test loss: 2.0444708 (new best train)
Epoch 3250, LR: [0.0005]: Train loss: 0.9416685, Test loss: 2.0266171
Epoch 3260, LR: [0.0005]: Train loss: 0.9301884, Test loss: 2.0109840 (new best train)
Epoch 3270, LR: [0.0005]: Train loss: 0.9428371, Test loss: 1.9921319
Epoch 3280, LR: [0.0005]: Train loss: 0.9385097, Test loss: 1.9803854
Epoch 3290, LR: [0.0005]: Train loss: 0.9351186, Test loss: 1.9946780
Epoch 3300, LR: [0.0005]: Train loss: 0.9405781, Test loss: 2.0201396
Epoch 3310, LR: [0.0005]: Train loss: 0.9288083, Test loss: 1.9948550 (new best train)
Epoch 3320, LR: [0.0005]: Train loss: 0.9301298, Test loss: 1.9785618
Epoch 3330, LR: [0.0005]: Train loss: 0.9379517, Test loss: 1.9880104
Epoch 3340, LR: [0.0005]: Train loss: 0.9301550, Test loss: 2.0560971
Epoch 3350, LR: [0.0005]: Train loss: 0.9340846, Test loss: 1.9818212
Epoch 3360, LR: [0.0005]: Train loss: 0.9280528, Test loss: 2.0244922 (new best train)
Epoch 3370, LR: [0.0005]: Train loss: 0.9261008, Test loss: 1.9690229 (new best train)
Epoch 3380, LR: [0.0005]: Train loss: 0.9238363, Test loss: 1.9745840 (new best train)
Epoch 3390, LR: [0.0005]: Train loss: 0.9287372, Test loss: 1.9944032
Epoch 3400, LR: [0.0005]: Train loss: 0.9268122, Test loss: 1.9761810
Epoch 3410, LR: [0.0005]: Train loss: 0.9260610, Test loss: 1.9681606
Epoch 3420, LR: [0.0005]: Train loss: 0.9286146, Test loss: 1.9843924
Epoch 3430, LR: [0.0005]: Train loss: 0.9255294, Test loss: 2.0313837
Epoch 3440, LR: [0.0005]: Train loss: 0.9388679, Test loss: 1.9937281
Epoch 3450, LR: [0.0005]: Train loss: 0.9228717, Test loss: 1.9657591 (new best train)
Epoch 3460, LR: [0.0005]: Train loss: 0.9260246, Test loss: 1.9892572
Epoch 3470, LR: [0.0005]: Train loss: 0.9263062, Test loss: 1.9774521
Epoch 3480, LR: [0.0005]: Train loss: 0.9289813, Test loss: 2.0283355
Epoch 3490, LR: [0.0005]: Train loss: 0.9235604, Test loss: 2.0269894
Epoch 3500, LR: [0.0005]: Train loss: 0.9224583, Test loss: 2.0229939 (new best train)
Epoch 3510, LR: [0.0005]: Train loss: 0.9265116, Test loss: 1.9915925
Epoch 3520, LR: [0.0005]: Train loss: 0.9250092, Test loss: 1.9760556
Epoch 3530, LR: [0.0005]: Train loss: 0.9310671, Test loss: 1.9896291
Epoch 3540, LR: [0.0005]: Train loss: 0.9249080, Test loss: 1.9780177
Epoch 3550, LR: [0.0005]: Train loss: 0.9162431, Test loss: 1.9719387 (new best train)
Epoch 3560, LR: [0.0005]: Train loss: 0.9231997, Test loss: 1.9904418
Epoch 3570, LR: [0.0005]: Train loss: 0.9232101, Test loss: 1.9856545
Epoch 3580, LR: [0.0005]: Train loss: 0.9223333, Test loss: 2.0285018
Epoch 3590, LR: [0.0005]: Train loss: 0.9210765, Test loss: 2.0230174
Epoch 3600, LR: [0.0005]: Train loss: 0.9234831, Test loss: 1.9911873
Epoch 3610, LR: [0.0005]: Train loss: 0.9217707, Test loss: 2.0617413
Epoch 3620, LR: [0.0005]: Train loss: 0.9314413, Test loss: 1.9902471
Epoch 3630, LR: [0.0005]: Train loss: 0.9172608, Test loss: 1.9705156
Epoch 3640, LR: [0.0005]: Train loss: 0.9211035, Test loss: 1.9696803
Epoch 3650, LR: [0.0005]: Train loss: 0.9146102, Test loss: 1.9774506 (new best train)
Epoch 3660, LR: [0.0005]: Train loss: 0.9208530, Test loss: 1.9625411
Epoch 3670, LR: [0.0005]: Train loss: 0.9173192, Test loss: 1.9872119
Epoch 3680, LR: [0.0005]: Train loss: 0.9151455, Test loss: 1.9944805
Epoch 3690, LR: [0.0005]: Train loss: 0.9129829, Test loss: 1.9750986 (new best train)
Epoch 3700, LR: [0.0005]: Train loss: 0.9207159, Test loss: 1.9740211
Epoch 3710, LR: [0.0005]: Train loss: 0.9193761, Test loss: 1.9732394
Epoch 3720, LR: [0.0005]: Train loss: 0.9246360, Test loss: 1.9851901
Epoch 3730, LR: [0.0005]: Train loss: 0.9314996, Test loss: 2.0067620
Epoch 3740, LR: [0.0005]: Train loss: 0.9072793, Test loss: 1.9946102 (new best train)
Epoch 3750, LR: [0.0005]: Train loss: 0.9187811, Test loss: 2.0064295
Epoch 3760, LR: [0.0005]: Train loss: 0.9227404, Test loss: 2.0004258
Epoch 3770, LR: [0.0005]: Train loss: 0.9185292, Test loss: 1.9905215
Epoch 3780, LR: [0.0005]: Train loss: 0.9090835, Test loss: 1.9788231
Epoch 3790, LR: [0.0005]: Train loss: 0.9124656, Test loss: 1.9744773
Epoch 3800, LR: [0.0005]: Train loss: 0.9110162, Test loss: 1.9760544
Epoch 3810, LR: [0.0005]: Train loss: 0.9123953, Test loss: 1.9638951
Epoch 3820, LR: [0.0005]: Train loss: 0.9058994, Test loss: 1.9722045 (new best train)
Epoch 3830, LR: [0.0005]: Train loss: 0.9126123, Test loss: 1.9872991
Epoch 3840, LR: [0.0005]: Train loss: 0.9121255, Test loss: 1.9965206
Epoch 3850, LR: [0.0005]: Train loss: 0.9121105, Test loss: 1.9723092
Epoch 3860, LR: [0.0005]: Train loss: 0.9186430, Test loss: 1.9756663
Epoch 3870, LR: [0.0005]: Train loss: 0.9179222, Test loss: 1.9775045
Epoch 3880, LR: [0.0005]: Train loss: 0.9137743, Test loss: 2.0544231
Epoch 3890, LR: [0.0005]: Train loss: 0.9165613, Test loss: 2.0238320
Epoch 3900, LR: [0.0005]: Train loss: 0.9132828, Test loss: 1.9920856
Epoch 3910, LR: [0.0005]: Train loss: 0.9058507, Test loss: 1.9962902
Epoch 3920, LR: [0.0005]: Train loss: 0.9135462, Test loss: 1.9663507
Epoch 3930, LR: [0.00025]: Train loss: 0.9089306, Test loss: 1.9910236
Epoch 3940, LR: [0.00025]: Train loss: 0.8921755, Test loss: 1.9745394 (new best train)
Epoch 3950, LR: [0.00025]: Train loss: 0.8904114, Test loss: 1.9641931 (new best train)
Epoch 3960, LR: [0.00025]: Train loss: 0.8937244, Test loss: 1.9819277
Epoch 3970, LR: [0.00025]: Train loss: 0.8981783, Test loss: 1.9737273
Epoch 3980, LR: [0.00025]: Train loss: 0.8951269, Test loss: 1.9940617
Epoch 3990, LR: [0.00025]: Train loss: 0.8933360, Test loss: 1.9677608
Epoch 4000, LR: [0.00025]: Train loss: 0.8916054, Test loss: 1.9621417
Epoch 4010, LR: [0.00025]: Train loss: 0.8905289, Test loss: 1.9776939
Epoch 4020, LR: [0.00025]: Train loss: 0.8896518, Test loss: 1.9765139 (new best train)
Epoch 4030, LR: [0.00025]: Train loss: 0.8925482, Test loss: 2.0076713
Epoch 4040, LR: [0.00025]: Train loss: 0.8929579, Test loss: 1.9835867
Epoch 4050, LR: [0.00025]: Train loss: 0.8930534, Test loss: 1.9686254
Epoch 4060, LR: [0.00025]: Train loss: 0.8942263, Test loss: 1.9869657
Epoch 4070, LR: [0.00025]: Train loss: 0.8881909, Test loss: 1.9710932 (new best train)
Epoch 4080, LR: [0.00025]: Train loss: 0.8968503, Test loss: 1.9951253
Epoch 4090, LR: [0.00025]: Train loss: 0.8950253, Test loss: 1.9580956
Epoch 4100, LR: [0.00025]: Train loss: 0.8919793, Test loss: 1.9687429
Epoch 4110, LR: [0.00025]: Train loss: 0.8902355, Test loss: 1.9639633
Epoch 4120, LR: [0.00025]: Train loss: 0.8921524, Test loss: 1.9855152
Epoch 4130, LR: [0.00025]: Train loss: 0.8888917, Test loss: 1.9710207
Epoch 4140, LR: [0.00025]: Train loss: 0.8944086, Test loss: 1.9678571
Epoch 4150, LR: [0.00025]: Train loss: 0.8901213, Test loss: 2.0031846
Epoch 4160, LR: [0.00025]: Train loss: 0.8935941, Test loss: 1.9835410
Epoch 4170, LR: [0.00025]: Train loss: 0.8872765, Test loss: 1.9717794 (new best train)
Epoch 4180, LR: [0.00025]: Train loss: 0.8878844, Test loss: 2.0207734
Epoch 4190, LR: [0.00025]: Train loss: 0.8864446, Test loss: 1.9836389 (new best train)
Epoch 4200, LR: [0.00025]: Train loss: 0.8946655, Test loss: 1.9707080
Epoch 4210, LR: [0.00025]: Train loss: 0.8854841, Test loss: 1.9667172 (new best train)
Epoch 4220, LR: [0.00025]: Train loss: 0.8914194, Test loss: 1.9712999
Epoch 4230, LR: [0.00025]: Train loss: 0.8869082, Test loss: 1.9586822
Epoch 4240, LR: [0.00025]: Train loss: 0.8847852, Test loss: 1.9596216 (new best train)
Epoch 4250, LR: [0.00025]: Train loss: 0.8859140, Test loss: 1.9724879
Epoch 4260, LR: [0.00025]: Train loss: 0.8870991, Test loss: 1.9664917
Epoch 4270, LR: [0.00025]: Train loss: 0.8815213, Test loss: 1.9586462 (new best train)
Epoch 4280, LR: [0.00025]: Train loss: 0.8826379, Test loss: 1.9692755
Epoch 4290, LR: [0.00025]: Train loss: 0.8891257, Test loss: 1.9719073
Epoch 4300, LR: [0.00025]: Train loss: 0.8831406, Test loss: 1.9644238
Epoch 4310, LR: [0.00025]: Train loss: 0.8865534, Test loss: 1.9645457
Epoch 4320, LR: [0.00025]: Train loss: 0.8860110, Test loss: 1.9686801
Epoch 4330, LR: [0.00025]: Train loss: 0.8918651, Test loss: 1.9559564
Epoch 4340, LR: [0.00025]: Train loss: 0.8840016, Test loss: 1.9983457
Epoch 4350, LR: [0.00025]: Train loss: 0.8835458, Test loss: 1.9750063
Epoch 4360, LR: [0.00025]: Train loss: 0.8851469, Test loss: 1.9616583
Epoch 4370, LR: [0.00025]: Train loss: 0.8800945, Test loss: 1.9706261 (new best train)
Epoch 4380, LR: [0.00025]: Train loss: 0.8847883, Test loss: 1.9966895
Epoch 4390, LR: [0.00025]: Train loss: 0.8874339, Test loss: 1.9744862
Epoch 4400, LR: [0.00025]: Train loss: 0.8853782, Test loss: 1.9701411
Epoch 4410, LR: [0.00025]: Train loss: 0.8854335, Test loss: 1.9658314
Epoch 4420, LR: [0.00025]: Train loss: 0.8808052, Test loss: 1.9712303
Epoch 4430, LR: [0.00025]: Train loss: 0.8914717, Test loss: 1.9869995
Epoch 4440, LR: [0.00025]: Train loss: 0.8835521, Test loss: 1.9678983
Epoch 4450, LR: [0.00025]: Train loss: 0.8834316, Test loss: 1.9811414
Epoch 4460, LR: [0.00025]: Train loss: 0.8830192, Test loss: 1.9719484
Epoch 4470, LR: [0.00025]: Train loss: 0.8789781, Test loss: 1.9737445 (new best train)
Epoch 4480, LR: [0.00025]: Train loss: 0.8880511, Test loss: 2.0095530
Epoch 4490, LR: [0.00025]: Train loss: 0.8852237, Test loss: 1.9662012
Epoch 4500, LR: [0.00025]: Train loss: 0.8822380, Test loss: 1.9690468
Epoch 4510, LR: [0.00025]: Train loss: 0.8802405, Test loss: 1.9623346
Epoch 4520, LR: [0.00025]: Train loss: 0.8799866, Test loss: 1.9539763
Epoch 4530, LR: [0.00025]: Train loss: 0.8780562, Test loss: 1.9802044 (new best train)
Epoch 4540, LR: [0.00025]: Train loss: 0.8809867, Test loss: 1.9795770
Epoch 4550, LR: [0.00025]: Train loss: 0.8793264, Test loss: 1.9739950
Epoch 4560, LR: [0.00025]: Train loss: 0.8784837, Test loss: 2.0039679
Epoch 4570, LR: [0.00025]: Train loss: 0.8804010, Test loss: 1.9632444
Epoch 4580, LR: [0.00025]: Train loss: 0.8762721, Test loss: 1.9727440 (new best train)
Epoch 4590, LR: [0.00025]: Train loss: 0.8821751, Test loss: 1.9647575
Epoch 4600, LR: [0.00025]: Train loss: 0.8793699, Test loss: 1.9717858
Epoch 4610, LR: [0.00025]: Train loss: 0.8788979, Test loss: 2.0093986
Epoch 4620, LR: [0.00025]: Train loss: 0.8905122, Test loss: 1.9644235
Epoch 4630, LR: [0.00025]: Train loss: 0.8778048, Test loss: 1.9619923
Epoch 4640, LR: [0.00025]: Train loss: 0.8796808, Test loss: 1.9962141
Epoch 4650, LR: [0.00025]: Train loss: 0.8934032, Test loss: 1.9773480
Epoch 4660, LR: [0.00025]: Train loss: 0.8808330, Test loss: 1.9885358
Epoch 4670, LR: [0.00025]: Train loss: 0.8744557, Test loss: 1.9694205 (new best train)
Epoch 4680, LR: [0.00025]: Train loss: 0.8763847, Test loss: 1.9615187
Epoch 4690, LR: [0.00025]: Train loss: 0.8814252, Test loss: 1.9692593
Epoch 4700, LR: [0.00025]: Train loss: 0.8818294, Test loss: 1.9613512
Epoch 4710, LR: [0.00025]: Train loss: 0.8758807, Test loss: 1.9620976
Epoch 4720, LR: [0.00025]: Train loss: 0.8725971, Test loss: 1.9791583 (new best train)
Epoch 4730, LR: [0.00025]: Train loss: 0.8769233, Test loss: 1.9537941
Epoch 4740, LR: [0.00025]: Train loss: 0.8761630, Test loss: 1.9699282
Epoch 4750, LR: [0.00025]: Train loss: 0.8746677, Test loss: 1.9823751
Epoch 4760, LR: [0.00025]: Train loss: 0.8797580, Test loss: 2.0008475
Epoch 4770, LR: [0.00025]: Train loss: 0.8732920, Test loss: 1.9580443
Epoch 4780, LR: [0.00025]: Train loss: 0.8855909, Test loss: 1.9469521
Epoch 4790, LR: [0.00025]: Train loss: 0.8743892, Test loss: 1.9674312
Epoch 4800, LR: [0.00025]: Train loss: 0.8732417, Test loss: 1.9759604
Epoch 4810, LR: [0.00025]: Train loss: 0.8726725, Test loss: 1.9474795
Epoch 4820, LR: [0.00025]: Train loss: 0.8783893, Test loss: 1.9555046
Epoch 4830, LR: [0.00025]: Train loss: 0.8697102, Test loss: 1.9559323 (new best train)
Epoch 4840, LR: [0.00025]: Train loss: 0.8708112, Test loss: 1.9551619
Epoch 4850, LR: [0.00025]: Train loss: 0.8769111, Test loss: 1.9497849
Epoch 4860, LR: [0.00025]: Train loss: 0.8736267, Test loss: 1.9599335
Epoch 4870, LR: [0.00025]: Train loss: 0.8780656, Test loss: 1.9814864
Epoch 4880, LR: [0.00025]: Train loss: 0.8723964, Test loss: 1.9529244
Epoch 4890, LR: [0.00025]: Train loss: 0.8763742, Test loss: 1.9699837
Epoch 4900, LR: [0.00025]: Train loss: 0.8765380, Test loss: 1.9737066
Epoch 4910, LR: [0.00025]: Train loss: 0.8740115, Test loss: 1.9644402
Epoch 4920, LR: [0.00025]: Train loss: 0.8677677, Test loss: 1.9625280 (new best train)
Epoch 4930, LR: [0.00025]: Train loss: 0.8769693, Test loss: 1.9586773
Epoch 4940, LR: [0.00025]: Train loss: 0.8773593, Test loss: 1.9617520
Epoch 4950, LR: [0.00025]: Train loss: 0.8682757, Test loss: 1.9955312
Epoch 4960, LR: [0.00025]: Train loss: 0.8725394, Test loss: 1.9727050
Epoch 4970, LR: [0.00025]: Train loss: 0.8742063, Test loss: 1.9729520
Epoch 4980, LR: [0.00025]: Train loss: 0.8722845, Test loss: 1.9524308
Epoch 4990, LR: [0.00025]: Train loss: 0.8722008, Test loss: 1.9545112
Epoch 5000, LR: [0.00025]: Train loss: 0.8730346, Test loss: 1.9674307
Epoch 5010, LR: [0.00025]: Train loss: 0.8751227, Test loss: 1.9590405
Epoch 5020, LR: [0.00025]: Train loss: 0.8676298, Test loss: 1.9469949 (new best train)
Epoch 5030, LR: [0.00025]: Train loss: 0.8704163, Test loss: 1.9763712
Epoch 5040, LR: [0.00025]: Train loss: 0.8740919, Test loss: 1.9591447
Epoch 5050, LR: [0.00025]: Train loss: 0.8722782, Test loss: 1.9519569
Epoch 5060, LR: [0.00025]: Train loss: 0.8749988, Test loss: 2.0308747
Epoch 5070, LR: [0.00025]: Train loss: 0.8829429, Test loss: 1.9546951
Epoch 5080, LR: [0.00025]: Train loss: 0.8643016, Test loss: 1.9630581 (new best train)
Epoch 5090, LR: [0.00025]: Train loss: 0.8664327, Test loss: 1.9491487
Epoch 5100, LR: [0.00025]: Train loss: 0.8779713, Test loss: 2.0008910
Epoch 5110, LR: [0.00025]: Train loss: 0.8737629, Test loss: 1.9525061
Epoch 5120, LR: [0.00025]: Train loss: 0.8750744, Test loss: 1.9478432
Epoch 5130, LR: [0.00025]: Train loss: 0.8676744, Test loss: 1.9690419
Epoch 5140, LR: [0.00025]: Train loss: 0.8690093, Test loss: 1.9548631
Epoch 5150, LR: [0.00025]: Train loss: 0.8658828, Test loss: 1.9483138
Epoch 5160, LR: [0.00025]: Train loss: 0.8668064, Test loss: 2.0328301
Epoch 5170, LR: [0.00025]: Train loss: 0.8727450, Test loss: 1.9747626
Epoch 5180, LR: [0.00025]: Train loss: 0.8817027, Test loss: 1.9887501
Epoch 5190, LR: [0.000125]: Train loss: 0.8765705, Test loss: 1.9649220
Epoch 5200, LR: [0.000125]: Train loss: 0.8577636, Test loss: 1.9630344 (new best train)
Epoch 5210, LR: [0.000125]: Train loss: 0.8591397, Test loss: 1.9542084
Epoch 5220, LR: [0.000125]: Train loss: 0.8569538, Test loss: 1.9608346 (new best train)
Epoch 5230, LR: [0.000125]: Train loss: 0.8566946, Test loss: 1.9500706 (new best train)
Epoch 5240, LR: [0.000125]: Train loss: 0.8564897, Test loss: 1.9640019 (new best train)
Epoch 5250, LR: [0.000125]: Train loss: 0.8548616, Test loss: 1.9541996 (new best train)
Epoch 5260, LR: [0.000125]: Train loss: 0.8555688, Test loss: 1.9584351
Epoch 5270, LR: [0.000125]: Train loss: 0.8571725, Test loss: 1.9505796
Epoch 5280, LR: [0.000125]: Train loss: 0.8569060, Test loss: 1.9504905
Epoch 5290, LR: [0.000125]: Train loss: 0.8564575, Test loss: 1.9484574
Epoch 5300, LR: [0.000125]: Train loss: 0.8563065, Test loss: 1.9500654
Epoch 5310, LR: [0.000125]: Train loss: 0.8579232, Test loss: 1.9561082
Epoch 5320, LR: [0.000125]: Train loss: 0.8561105, Test loss: 1.9520062
Epoch 5330, LR: [0.000125]: Train loss: 0.8576457, Test loss: 1.9980864
Epoch 5340, LR: [0.000125]: Train loss: 0.8599090, Test loss: 1.9640381
Epoch 5350, LR: [0.000125]: Train loss: 0.8567655, Test loss: 1.9510960
Epoch 5360, LR: [6.25e-05]: Train loss: 0.8571877, Test loss: 1.9550858
Epoch 5370, LR: [6.25e-05]: Train loss: 0.8536534, Test loss: 1.9467655 (new best train)
Epoch 5380, LR: [6.25e-05]: Train loss: 0.8509812, Test loss: 1.9491996 (new best train)
Epoch 5390, LR: [6.25e-05]: Train loss: 0.8528065, Test loss: 1.9527954
Epoch 5400, LR: [6.25e-05]: Train loss: 0.8512701, Test loss: 1.9580124
Epoch 5410, LR: [6.25e-05]: Train loss: 0.8508835, Test loss: 1.9518303
Epoch 5420, LR: [6.25e-05]: Train loss: 0.8518336, Test loss: 1.9545439
Epoch 5430, LR: [6.25e-05]: Train loss: 0.8507099, Test loss: 1.9527736 (new best train)
Epoch 5440, LR: [6.25e-05]: Train loss: 0.8510773, Test loss: 1.9672832
Epoch 5450, LR: [6.25e-05]: Train loss: 0.8504680, Test loss: 1.9599387 (new best train)
Epoch 5460, LR: [6.25e-05]: Train loss: 0.8514633, Test loss: 1.9561454
Epoch 5470, LR: [6.25e-05]: Train loss: 0.8510667, Test loss: 1.9526065
Epoch 5480, LR: [6.25e-05]: Train loss: 0.8504681, Test loss: 1.9506598
Epoch 5490, LR: [6.25e-05]: Train loss: 0.8516860, Test loss: 1.9613252
Epoch 5500, LR: [6.25e-05]: Train loss: 0.8514690, Test loss: 1.9677057
Epoch 5510, LR: [6.25e-05]: Train loss: 0.8546422, Test loss: 1.9572992
Epoch 5520, LR: [6.25e-05]: Train loss: 0.8532065, Test loss: 1.9610475
Epoch 5530, LR: [6.25e-05]: Train loss: 0.8514617, Test loss: 1.9502405
Epoch 5540, LR: [6.25e-05]: Train loss: 0.8505620, Test loss: 1.9602846
Epoch 5550, LR: [6.25e-05]: Train loss: 0.8510763, Test loss: 1.9566577
Epoch 5560, LR: [3.125e-05]: Train loss: 0.8505292, Test loss: 1.9557775
Epoch 5570, LR: [3.125e-05]: Train loss: 0.8491600, Test loss: 1.9515917 (new best train)
Epoch 5580, LR: [3.125e-05]: Train loss: 0.8478146, Test loss: 1.9532683 (new best train)
Epoch 5590, LR: [3.125e-05]: Train loss: 0.8474849, Test loss: 1.9539081 (new best train)
Epoch 5600, LR: [3.125e-05]: Train loss: 0.8473749, Test loss: 1.9568013 (new best train)
Epoch 5610, LR: [3.125e-05]: Train loss: 0.8476383, Test loss: 1.9586283
Epoch 5620, LR: [3.125e-05]: Train loss: 0.8476228, Test loss: 1.9545819
Epoch 5630, LR: [3.125e-05]: Train loss: 0.8473584, Test loss: 1.9503917
Epoch 5640, LR: [3.125e-05]: Train loss: 0.8479811, Test loss: 1.9583754
Epoch 5650, LR: [3.125e-05]: Train loss: 0.8475472, Test loss: 1.9495352
Epoch 5660, LR: [3.125e-05]: Train loss: 0.8476435, Test loss: 1.9513042
Epoch 5670, LR: [3.125e-05]: Train loss: 0.8475392, Test loss: 1.9682391
Epoch 5680, LR: [3.125e-05]: Train loss: 0.8489595, Test loss: 1.9600492
Epoch 5690, LR: [3.125e-05]: Train loss: 0.8469350, Test loss: 1.9516803 (new best train)
Epoch 5700, LR: [3.125e-05]: Train loss: 0.8468802, Test loss: 1.9519426
Epoch 5710, LR: [3.125e-05]: Train loss: 0.8470644, Test loss: 1.9535710
Epoch 5720, LR: [3.125e-05]: Train loss: 0.8466611, Test loss: 1.9541646 (new best train)
Epoch 5730, LR: [3.125e-05]: Train loss: 0.8466200, Test loss: 1.9513558
Epoch 5740, LR: [3.125e-05]: Train loss: 0.8470333, Test loss: 1.9639843
Epoch 5750, LR: [3.125e-05]: Train loss: 0.8465680, Test loss: 1.9551422
Epoch 5760, LR: [3.125e-05]: Train loss: 0.8472213, Test loss: 1.9544113
Epoch 5770, LR: [3.125e-05]: Train loss: 0.8473538, Test loss: 1.9569167
Epoch 5780, LR: [3.125e-05]: Train loss: 0.8468475, Test loss: 1.9501226
Epoch 5790, LR: [3.125e-05]: Train loss: 0.8472238, Test loss: 1.9504468
Epoch 5800, LR: [3.125e-05]: Train loss: 0.8455609, Test loss: 1.9539862 (new best train)
Epoch 5810, LR: [3.125e-05]: Train loss: 0.8471480, Test loss: 1.9482135
Epoch 5820, LR: [3.125e-05]: Train loss: 0.8465331, Test loss: 1.9550056
Epoch 5830, LR: [3.125e-05]: Train loss: 0.8461405, Test loss: 1.9507067
Epoch 5840, LR: [3.125e-05]: Train loss: 0.8459893, Test loss: 1.9520981
Epoch 5850, LR: [3.125e-05]: Train loss: 0.8464911, Test loss: 1.9563968
Epoch 5860, LR: [3.125e-05]: Train loss: 0.8460661, Test loss: 1.9533977
Epoch 5870, LR: [3.125e-05]: Train loss: 0.8472331, Test loss: 1.9572731
Epoch 5880, LR: [3.125e-05]: Train loss: 0.8462256, Test loss: 1.9490166
Epoch 5890, LR: [3.125e-05]: Train loss: 0.8461178, Test loss: 1.9585971
Epoch 5900, LR: [3.125e-05]: Train loss: 0.8457470, Test loss: 1.9535912
Epoch 5910, LR: [1.5625e-05]: Train loss: 0.8465567, Test loss: 1.9487819
Epoch 5920, LR: [1.5625e-05]: Train loss: 0.8444574, Test loss: 1.9479982 (new best train)
Epoch 5930, LR: [1.5625e-05]: Train loss: 0.8439631, Test loss: 1.9521949 (new best train)
Epoch 5940, LR: [1.5625e-05]: Train loss: 0.8443317, Test loss: 1.9486589
Epoch 5950, LR: [1.5625e-05]: Train loss: 0.8442250, Test loss: 1.9538374
Epoch 5960, LR: [1.5625e-05]: Train loss: 0.8454511, Test loss: 1.9471625
Epoch 5970, LR: [1.5625e-05]: Train loss: 0.8442271, Test loss: 1.9547731
Epoch 5980, LR: [1.5625e-05]: Train loss: 0.8443283, Test loss: 1.9546101
Epoch 5990, LR: [1.5625e-05]: Train loss: 0.8443252, Test loss: 1.9472507
Epoch 6000, LR: [1.5625e-05]: Train loss: 0.8438037, Test loss: 1.9491032 (new best train)
Epoch 6010, LR: [1.5625e-05]: Train loss: 0.8436472, Test loss: 1.9533051 (new best train)
Epoch 6020, LR: [1.5625e-05]: Train loss: 0.8441576, Test loss: 1.9547075
Epoch 6030, LR: [1.5625e-05]: Train loss: 0.8441272, Test loss: 1.9514629
Epoch 6040, LR: [1.5625e-05]: Train loss: 0.8435601, Test loss: 1.9487797
Epoch 6050, LR: [1.5625e-05]: Train loss: 0.8447980, Test loss: 1.9528707
Epoch 6060, LR: [1.5625e-05]: Train loss: 0.8438524, Test loss: 1.9526438
Epoch 6070, LR: [1.5625e-05]: Train loss: 0.8437458, Test loss: 1.9523449
Epoch 6080, LR: [1.5625e-05]: Train loss: 0.8443166, Test loss: 1.9494304
Epoch 6090, LR: [1.5625e-05]: Train loss: 0.8434991, Test loss: 1.9497208 (new best train)
Epoch 6100, LR: [1.5625e-05]: Train loss: 0.8439294, Test loss: 1.9529142
Epoch 6110, LR: [1.5625e-05]: Train loss: 0.8434610, Test loss: 1.9512692
Epoch 6120, LR: [1.5625e-05]: Train loss: 0.8445654, Test loss: 1.9504758
Epoch 6130, LR: [1.5625e-05]: Train loss: 0.8436454, Test loss: 1.9483892
Epoch 6140, LR: [1.5625e-05]: Train loss: 0.8444094, Test loss: 1.9523849
Epoch 6150, LR: [1.5625e-05]: Train loss: 0.8437339, Test loss: 1.9483299
Epoch 6160, LR: [1.5625e-05]: Train loss: 0.8451198, Test loss: 1.9502211
Epoch 6170, LR: [1.5625e-05]: Train loss: 0.8436940, Test loss: 1.9494421
Epoch 6180, LR: [1.5625e-05]: Train loss: 0.8432103, Test loss: 1.9531005 (new best train)
Epoch 6190, LR: [1.5625e-05]: Train loss: 0.8436670, Test loss: 1.9505793
Epoch 6200, LR: [1.5625e-05]: Train loss: 0.8432831, Test loss: 1.9526750
Epoch 6210, LR: [1.5625e-05]: Train loss: 0.8435146, Test loss: 1.9532067
Epoch 6220, LR: [1.5625e-05]: Train loss: 0.8442953, Test loss: 1.9601981
Epoch 6230, LR: [1.5625e-05]: Train loss: 0.8432131, Test loss: 1.9503110
Epoch 6240, LR: [1.5625e-05]: Train loss: 0.8441289, Test loss: 1.9511847
Epoch 6250, LR: [1.5625e-05]: Train loss: 0.8430229, Test loss: 1.9554276 (new best train)
Epoch 6260, LR: [1.5625e-05]: Train loss: 0.8434486, Test loss: 1.9500803
Epoch 6270, LR: [1.5625e-05]: Train loss: 0.8431379, Test loss: 1.9499088
Epoch 6280, LR: [1.5625e-05]: Train loss: 0.8434255, Test loss: 1.9506327
Epoch 6290, LR: [1.5625e-05]: Train loss: 0.8430584, Test loss: 1.9516348
Epoch 6300, LR: [1.5625e-05]: Train loss: 0.8431811, Test loss: 1.9488212
Epoch 6310, LR: [1.5625e-05]: Train loss: 0.8432531, Test loss: 1.9477429
Epoch 6320, LR: [1.5625e-05]: Train loss: 0.8440737, Test loss: 1.9524501
Epoch 6330, LR: [1.5625e-05]: Train loss: 0.8435739, Test loss: 1.9569620
Epoch 6340, LR: [1.5625e-05]: Train loss: 0.8432469, Test loss: 1.9513017
Epoch 6350, LR: [1.5625e-05]: Train loss: 0.8429677, Test loss: 1.9527212
Epoch 6360, LR: [1.5625e-05]: Train loss: 0.8426742, Test loss: 1.9574950 (new best train)
Epoch 6370, LR: [1.5625e-05]: Train loss: 0.8430492, Test loss: 1.9573525
Epoch 6380, LR: [1.5625e-05]: Train loss: 0.8430586, Test loss: 1.9504375
Epoch 6390, LR: [1.5625e-05]: Train loss: 0.8431716, Test loss: 1.9505896
Epoch 6400, LR: [1.5625e-05]: Train loss: 0.8425578, Test loss: 1.9501881 (new best train)
Epoch 6410, LR: [1.5625e-05]: Train loss: 0.8426428, Test loss: 1.9515270
Epoch 6420, LR: [1.5625e-05]: Train loss: 0.8427170, Test loss: 1.9544221
Epoch 6430, LR: [1.5625e-05]: Train loss: 0.8431331, Test loss: 1.9528289
Epoch 6440, LR: [1.5625e-05]: Train loss: 0.8432087, Test loss: 1.9563514
Epoch 6450, LR: [1.5625e-05]: Train loss: 0.8438036, Test loss: 1.9523834
Epoch 6460, LR: [1.5625e-05]: Train loss: 0.8429411, Test loss: 1.9500406
Epoch 6470, LR: [1.5625e-05]: Train loss: 0.8428554, Test loss: 1.9508490
Epoch 6480, LR: [1.5625e-05]: Train loss: 0.8428002, Test loss: 1.9511253
Epoch 6490, LR: [1.5625e-05]: Train loss: 0.8423108, Test loss: 1.9488670 (new best train)
Epoch 6500, LR: [1.5625e-05]: Train loss: 0.8430116, Test loss: 1.9523947
Epoch 6510, LR: [1.5625e-05]: Train loss: 0.8424055, Test loss: 1.9510838
Epoch 6520, LR: [1.5625e-05]: Train loss: 0.8422048, Test loss: 1.9480247 (new best train)
Epoch 6530, LR: [1.5625e-05]: Train loss: 0.8427849, Test loss: 1.9485996
Epoch 6540, LR: [1.5625e-05]: Train loss: 0.8423862, Test loss: 1.9541575
Epoch 6550, LR: [1.5625e-05]: Train loss: 0.8423087, Test loss: 1.9491147
Epoch 6560, LR: [1.5625e-05]: Train loss: 0.8431852, Test loss: 1.9534852
Epoch 6570, LR: [1.5625e-05]: Train loss: 0.8421745, Test loss: 1.9558411
Epoch 6580, LR: [1.5625e-05]: Train loss: 0.8426983, Test loss: 1.9506388
Epoch 6590, LR: [1.5625e-05]: Train loss: 0.8427211, Test loss: 1.9518203
Epoch 6600, LR: [1.5625e-05]: Train loss: 0.8422046, Test loss: 1.9614738
Epoch 6610, LR: [1.5625e-05]: Train loss: 0.8426678, Test loss: 1.9528797
Epoch 6620, LR: [1.5625e-05]: Train loss: 0.8425406, Test loss: 1.9511263
Epoch 6630, LR: [7.8125e-06]: Train loss: 0.8425632, Test loss: 1.9500395
Epoch 6640, LR: [7.8125e-06]: Train loss: 0.8412500, Test loss: 1.9487838 (new best train)
Epoch 6650, LR: [7.8125e-06]: Train loss: 0.8413173, Test loss: 1.9501867
Epoch 6660, LR: [7.8125e-06]: Train loss: 0.8413756, Test loss: 1.9503161
Epoch 6670, LR: [7.8125e-06]: Train loss: 0.8416844, Test loss: 1.9522596
Epoch 6680, LR: [7.8125e-06]: Train loss: 0.8412695, Test loss: 1.9497359
Epoch 6690, LR: [7.8125e-06]: Train loss: 0.8410624, Test loss: 1.9509406 (new best train)
Epoch 6700, LR: [7.8125e-06]: Train loss: 0.8410289, Test loss: 1.9523474
Epoch 6710, LR: [7.8125e-06]: Train loss: 0.8412247, Test loss: 1.9471892
Epoch 6720, LR: [7.8125e-06]: Train loss: 0.8413223, Test loss: 1.9510029
Epoch 6730, LR: [7.8125e-06]: Train loss: 0.8410998, Test loss: 1.9492797
Epoch 6740, LR: [7.8125e-06]: Train loss: 0.8411728, Test loss: 1.9493816
Epoch 6750, LR: [7.8125e-06]: Train loss: 0.8415214, Test loss: 1.9544417
Epoch 6760, LR: [7.8125e-06]: Train loss: 0.8410599, Test loss: 1.9482885
Epoch 6770, LR: [7.8125e-06]: Train loss: 0.8411216, Test loss: 1.9497289
Epoch 6780, LR: [7.8125e-06]: Train loss: 0.8410470, Test loss: 1.9557184
Epoch 6790, LR: [7.8125e-06]: Train loss: 0.8415020, Test loss: 1.9496491
Epoch 6800, LR: [3.90625e-06]: Train loss: 0.8410765, Test loss: 1.9493532
Epoch 6810, LR: [3.90625e-06]: Train loss: 0.8406140, Test loss: 1.9503920 (new best train)
Epoch 6820, LR: [3.90625e-06]: Train loss: 0.8405559, Test loss: 1.9493053
Epoch 6830, LR: [3.90625e-06]: Train loss: 0.8405397, Test loss: 1.9493710
Epoch 6840, LR: [3.90625e-06]: Train loss: 0.8408300, Test loss: 1.9479240
Epoch 6850, LR: [3.90625e-06]: Train loss: 0.8406166, Test loss: 1.9505501
Epoch 6860, LR: [3.90625e-06]: Train loss: 0.8406372, Test loss: 1.9507508
Epoch 6870, LR: [3.90625e-06]: Train loss: 0.8403912, Test loss: 1.9504504 (new best train)
Epoch 6880, LR: [3.90625e-06]: Train loss: 0.8405438, Test loss: 1.9500038
Epoch 6890, LR: [3.90625e-06]: Train loss: 0.8404145, Test loss: 1.9483276
Epoch 6900, LR: [3.90625e-06]: Train loss: 0.8405473, Test loss: 1.9513404
Epoch 6910, LR: [3.90625e-06]: Train loss: 0.8403954, Test loss: 1.9499656
Epoch 6920, LR: [3.90625e-06]: Train loss: 0.8404871, Test loss: 1.9502388
Epoch 6930, LR: [3.90625e-06]: Train loss: 0.8404351, Test loss: 1.9482984
Epoch 6940, LR: [3.90625e-06]: Train loss: 0.8404194, Test loss: 1.9487991
Epoch 6950, LR: [3.90625e-06]: Train loss: 0.8404132, Test loss: 1.9513020
Epoch 6960, LR: [3.90625e-06]: Train loss: 0.8404276, Test loss: 1.9493859
Epoch 6970, LR: [3.90625e-06]: Train loss: 0.8404792, Test loss: 1.9509027
Epoch 6980, LR: [1.953125e-06]: Train loss: 0.8404266, Test loss: 1.9519617
Epoch 6990, LR: [1.953125e-06]: Train loss: 0.8401416, Test loss: 1.9508450 (new best train)
Epoch 7000, LR: [1.953125e-06]: Train loss: 0.8401541, Test loss: 1.9489172
Epoch 7010, LR: [1.953125e-06]: Train loss: 0.8401148, Test loss: 1.9503954
Epoch 7020, LR: [1.953125e-06]: Train loss: 0.8401141, Test loss: 1.9506709
Epoch 7030, LR: [1.953125e-06]: Train loss: 0.8400495, Test loss: 1.9514170
Epoch 7040, LR: [1.953125e-06]: Train loss: 0.8402184, Test loss: 1.9503347
Epoch 7050, LR: [1.953125e-06]: Train loss: 0.8400617, Test loss: 1.9505331
Epoch 7060, LR: [1.953125e-06]: Train loss: 0.8401679, Test loss: 1.9500150
Epoch 7070, LR: [1.953125e-06]: Train loss: 0.8401263, Test loss: 1.9490680
Epoch 7080, LR: [1.953125e-06]: Train loss: 0.8400957, Test loss: 1.9527163
Epoch 7090, LR: [1.953125e-06]: Train loss: 0.8401151, Test loss: 1.9516011
Epoch 7100, LR: [1.953125e-06]: Train loss: 0.8399944, Test loss: 1.9509887 (new best train)
Epoch 7110, LR: [1.953125e-06]: Train loss: 0.8401095, Test loss: 1.9501328
Epoch 7120, LR: [1.953125e-06]: Train loss: 0.8400905, Test loss: 1.9505727
Epoch 7130, LR: [1.953125e-06]: Train loss: 0.8401033, Test loss: 1.9502930
Epoch 7140, LR: [1.953125e-06]: Train loss: 0.8400935, Test loss: 1.9509808
Epoch 7150, LR: [1.953125e-06]: Train loss: 0.8402189, Test loss: 1.9510234
Epoch 7160, LR: [1.953125e-06]: Train loss: 0.8400343, Test loss: 1.9502429
Epoch 7170, LR: [1.953125e-06]: Train loss: 0.8400412, Test loss: 1.9495800
Epoch 7180, LR: [1.953125e-06]: Train loss: 0.8400267, Test loss: 1.9500546
Epoch 7190, LR: [1.953125e-06]: Train loss: 0.8400969, Test loss: 1.9504432
Epoch 7200, LR: [1.953125e-06]: Train loss: 0.8399910, Test loss: 1.9502807
Epoch 7210, LR: [9.765625e-07]: Train loss: 0.8400624, Test loss: 1.9499148
Epoch 7220, LR: [9.765625e-07]: Train loss: 0.8398564, Test loss: 1.9508523 (new best train)
Epoch 7230, LR: [9.765625e-07]: Train loss: 0.8398923, Test loss: 1.9504221
Epoch 7240, LR: [9.765625e-07]: Train loss: 0.8399537, Test loss: 1.9506711
Epoch 7250, LR: [9.765625e-07]: Train loss: 0.8398906, Test loss: 1.9505056
Epoch 7260, LR: [9.765625e-07]: Train loss: 0.8399055, Test loss: 1.9501885
Epoch 7270, LR: [9.765625e-07]: Train loss: 0.8398372, Test loss: 1.9508286
Epoch 7280, LR: [9.765625e-07]: Train loss: 0.8398804, Test loss: 1.9503812
Epoch 7290, LR: [9.765625e-07]: Train loss: 0.8398989, Test loss: 1.9498492
Epoch 7300, LR: [9.765625e-07]: Train loss: 0.8398615, Test loss: 1.9503547
Epoch 7310, LR: [9.765625e-07]: Train loss: 0.8398654, Test loss: 1.9507922
Epoch 7320, LR: [9.765625e-07]: Train loss: 0.8398213, Test loss: 1.9501871
Epoch 7330, LR: [4.8828125e-07]: Train loss: 0.8398665, Test loss: 1.9505026
Epoch 7340, LR: [4.8828125e-07]: Train loss: 0.8397929, Test loss: 1.9503519
Epoch 7350, LR: [4.8828125e-07]: Train loss: 0.8397874, Test loss: 1.9503088
Epoch 7360, LR: [4.8828125e-07]: Train loss: 0.8398189, Test loss: 1.9503520
Epoch 7370, LR: [4.8828125e-07]: Train loss: 0.8397795, Test loss: 1.9503995
Epoch 7380, LR: [4.8828125e-07]: Train loss: 0.8398035, Test loss: 1.9503729
Epoch 7390, LR: [4.8828125e-07]: Train loss: 0.8397533, Test loss: 1.9512452 (new best train)
Epoch 7400, LR: [4.8828125e-07]: Train loss: 0.8397408, Test loss: 1.9499855
Epoch 7410, LR: [4.8828125e-07]: Train loss: 0.8397611, Test loss: 1.9504298
Epoch 7420, LR: [4.8828125e-07]: Train loss: 0.8397664, Test loss: 1.9504617
Epoch 7430, LR: [4.8828125e-07]: Train loss: 0.8398247, Test loss: 1.9504226
Epoch 7440, LR: [4.8828125e-07]: Train loss: 0.8397506, Test loss: 1.9507414
Epoch 7450, LR: [4.8828125e-07]: Train loss: 0.8397463, Test loss: 1.9501471
Epoch 7460, LR: [4.8828125e-07]: Train loss: 0.8397688, Test loss: 1.9503720
Epoch 7470, LR: [4.8828125e-07]: Train loss: 0.8397744, Test loss: 1.9501957
Epoch 7480, LR: [4.8828125e-07]: Train loss: 0.8397537, Test loss: 1.9505830
Epoch 7490, LR: [4.8828125e-07]: Train loss: 0.8397830, Test loss: 1.9500971
Epoch 7500, LR: [2.44140625e-07]: Train loss: 0.8397769, Test loss: 1.9507344
Epoch 7510, LR: [2.44140625e-07]: Train loss: 0.8397183, Test loss: 1.9503218
Epoch 7520, LR: [2.44140625e-07]: Train loss: 0.8397272, Test loss: 1.9502920
Epoch 7530, LR: [2.44140625e-07]: Train loss: 0.8397409, Test loss: 1.9506129
Epoch 7540, LR: [2.44140625e-07]: Train loss: 0.8397117, Test loss: 1.9503493
Epoch 7550, LR: [2.44140625e-07]: Train loss: 0.8397026, Test loss: 1.9502037
Epoch 7560, LR: [2.44140625e-07]: Train loss: 0.8397053, Test loss: 1.9504995
Epoch 7570, LR: [2.44140625e-07]: Train loss: 0.8397058, Test loss: 1.9504574
Epoch 7580, LR: [2.44140625e-07]: Train loss: 0.8397280, Test loss: 1.9504736
Epoch 7590, LR: [2.44140625e-07]: Train loss: 0.8397186, Test loss: 1.9502349
20 * 10 epochs without STOP.TRAIN improvement, stopping. 
Best train perf: 0.8397533423105875, epoch: 7390
Total time: 2601.058443069458
