{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selective Long-Range Connections in Message Passing Neural Networks\n",
    "In this notebook, we will show that the message passing neural network (MPNN) can be improved upon for problems with a high problem-radius by using a last layer where some select nodes are connected over long distances to nodes otherwise unreachable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maxl/anaconda3/envs/selective-long-range-connection/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(0)\n",
    "\n",
    "from torch_geometric import nn as gnn\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.data import DataLoader\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load graph data\n",
    "from torch_geometric.datasets import TUDataset\n",
    "dataset = TUDataset(root='/tmp/MUTAG', name='MUTAG')\n",
    "\n",
    "# Split data into train and test sets\n",
    "train_dataset, test_dataset = train_test_split(dataset, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a model class using torch geometric\n",
    "class SimpleGNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_node_features, hidden_channels, dense_input, num_classes):\n",
    "        super(SimpleGNN, self).__init__()\n",
    "        \n",
    "        self.convs = nn.ModuleList()\n",
    "        self.convs.append(GCNConv(num_node_features, hidden_channels[0]))\n",
    "        for i in range(1, len(hidden_channels)):\n",
    "            self.convs.append(GCNConv(hidden_channels[i-1], hidden_channels[i]))\n",
    "        self.convs.append(GCNConv(hidden_channels[-1], dense_input))\n",
    "\n",
    "        self.dense = nn.Linear(dense_input, num_classes)\n",
    "\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "\n",
    "        for conv in self.convs[:-1]:\n",
    "            x = conv(x, edge_index)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, p=0.5, training=self.training)\n",
    "\n",
    "        x = self.convs[-1](x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = gnn.global_mean_pool(x, batch)\n",
    "\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.dense(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maxl/anaconda3/envs/selective-long-range-connection/lib/python3.10/site-packages/torch_geometric/deprecation.py:22: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Test Acc: 0.3158, Test Loss: 0.0372\n",
      "Epoch: 002, Test Acc: 0.6842, Test Loss: 0.0363\n",
      "Epoch: 003, Test Acc: 0.6842, Test Loss: 0.0352\n",
      "Epoch: 004, Test Acc: 0.6842, Test Loss: 0.0341\n",
      "Epoch: 005, Test Acc: 0.6842, Test Loss: 0.0331\n",
      "Epoch: 006, Test Acc: 0.6842, Test Loss: 0.0325\n",
      "Epoch: 007, Test Acc: 0.6842, Test Loss: 0.0324\n",
      "Epoch: 008, Test Acc: 0.6842, Test Loss: 0.0324\n",
      "Epoch: 009, Test Acc: 0.6842, Test Loss: 0.0323\n",
      "Epoch: 010, Test Acc: 0.6842, Test Loss: 0.0323\n",
      "Epoch: 011, Test Acc: 0.6842, Test Loss: 0.0323\n",
      "Epoch: 012, Test Acc: 0.6842, Test Loss: 0.0323\n",
      "Epoch: 013, Test Acc: 0.6842, Test Loss: 0.0322\n",
      "Epoch: 014, Test Acc: 0.6842, Test Loss: 0.0321\n",
      "Epoch: 015, Test Acc: 0.6842, Test Loss: 0.0320\n",
      "Epoch: 016, Test Acc: 0.6842, Test Loss: 0.0319\n",
      "Epoch: 017, Test Acc: 0.6842, Test Loss: 0.0318\n",
      "Epoch: 018, Test Acc: 0.6842, Test Loss: 0.0316\n",
      "Epoch: 019, Test Acc: 0.6842, Test Loss: 0.0315\n",
      "Epoch: 020, Test Acc: 0.6842, Test Loss: 0.0313\n",
      "Epoch: 021, Test Acc: 0.6842, Test Loss: 0.0311\n",
      "Epoch: 022, Test Acc: 0.6842, Test Loss: 0.0309\n",
      "Epoch: 023, Test Acc: 0.6842, Test Loss: 0.0307\n",
      "Epoch: 024, Test Acc: 0.6842, Test Loss: 0.0305\n",
      "Epoch: 025, Test Acc: 0.6842, Test Loss: 0.0302\n",
      "Epoch: 026, Test Acc: 0.6842, Test Loss: 0.0299\n",
      "Epoch: 027, Test Acc: 0.6842, Test Loss: 0.0295\n",
      "Epoch: 028, Test Acc: 0.6842, Test Loss: 0.0291\n",
      "Epoch: 029, Test Acc: 0.6842, Test Loss: 0.0288\n",
      "Epoch: 030, Test Acc: 0.6842, Test Loss: 0.0285\n",
      "Epoch: 031, Test Acc: 0.7368, Test Loss: 0.0281\n",
      "Epoch: 032, Test Acc: 0.7105, Test Loss: 0.0276\n",
      "Epoch: 033, Test Acc: 0.7632, Test Loss: 0.0272\n",
      "Epoch: 034, Test Acc: 0.8158, Test Loss: 0.0272\n",
      "Epoch: 035, Test Acc: 0.8158, Test Loss: 0.0266\n",
      "Epoch: 036, Test Acc: 0.8158, Test Loss: 0.0263\n",
      "Epoch: 037, Test Acc: 0.7895, Test Loss: 0.0262\n",
      "Epoch: 038, Test Acc: 0.7895, Test Loss: 0.0259\n",
      "Epoch: 039, Test Acc: 0.7895, Test Loss: 0.0258\n",
      "Epoch: 040, Test Acc: 0.8158, Test Loss: 0.0255\n",
      "Epoch: 041, Test Acc: 0.7895, Test Loss: 0.0253\n",
      "Epoch: 042, Test Acc: 0.7895, Test Loss: 0.0253\n",
      "Epoch: 043, Test Acc: 0.7895, Test Loss: 0.0251\n",
      "Epoch: 044, Test Acc: 0.8158, Test Loss: 0.0255\n",
      "Epoch: 045, Test Acc: 0.7895, Test Loss: 0.0250\n",
      "Epoch: 046, Test Acc: 0.8158, Test Loss: 0.0250\n",
      "Epoch: 047, Test Acc: 0.7895, Test Loss: 0.0249\n",
      "Epoch: 048, Test Acc: 0.8421, Test Loss: 0.0255\n",
      "Epoch: 049, Test Acc: 0.8158, Test Loss: 0.0248\n",
      "Epoch: 050, Test Acc: 0.8158, Test Loss: 0.0248\n",
      "Epoch: 051, Test Acc: 0.8421, Test Loss: 0.0256\n",
      "Epoch: 052, Test Acc: 0.8421, Test Loss: 0.0254\n",
      "Epoch: 053, Test Acc: 0.8158, Test Loss: 0.0247\n",
      "Epoch: 054, Test Acc: 0.8158, Test Loss: 0.0246\n",
      "Epoch: 055, Test Acc: 0.7895, Test Loss: 0.0246\n",
      "Epoch: 056, Test Acc: 0.8158, Test Loss: 0.0248\n",
      "Epoch: 057, Test Acc: 0.8158, Test Loss: 0.0246\n",
      "Epoch: 058, Test Acc: 0.8158, Test Loss: 0.0245\n",
      "Epoch: 059, Test Acc: 0.8158, Test Loss: 0.0245\n",
      "Epoch: 060, Test Acc: 0.8158, Test Loss: 0.0246\n",
      "Epoch: 061, Test Acc: 0.8158, Test Loss: 0.0247\n",
      "Epoch: 062, Test Acc: 0.8158, Test Loss: 0.0246\n",
      "Epoch: 063, Test Acc: 0.7895, Test Loss: 0.0244\n",
      "Epoch: 064, Test Acc: 0.8158, Test Loss: 0.0243\n",
      "Epoch: 065, Test Acc: 0.8158, Test Loss: 0.0244\n",
      "Epoch: 066, Test Acc: 0.8158, Test Loss: 0.0244\n",
      "Epoch: 067, Test Acc: 0.8158, Test Loss: 0.0243\n",
      "Epoch: 068, Test Acc: 0.8158, Test Loss: 0.0246\n",
      "Epoch: 069, Test Acc: 0.8158, Test Loss: 0.0243\n",
      "Epoch: 070, Test Acc: 0.8158, Test Loss: 0.0242\n",
      "Epoch: 071, Test Acc: 0.8158, Test Loss: 0.0243\n",
      "Epoch: 072, Test Acc: 0.8158, Test Loss: 0.0245\n",
      "Epoch: 073, Test Acc: 0.8421, Test Loss: 0.0242\n",
      "Epoch: 074, Test Acc: 0.8421, Test Loss: 0.0242\n",
      "Epoch: 075, Test Acc: 0.8158, Test Loss: 0.0242\n",
      "Epoch: 076, Test Acc: 0.8158, Test Loss: 0.0244\n",
      "Epoch: 077, Test Acc: 0.8158, Test Loss: 0.0243\n",
      "Epoch: 078, Test Acc: 0.8158, Test Loss: 0.0243\n",
      "Epoch: 079, Test Acc: 0.8158, Test Loss: 0.0243\n",
      "Epoch: 080, Test Acc: 0.8421, Test Loss: 0.0241\n",
      "Epoch: 081, Test Acc: 0.8158, Test Loss: 0.0239\n",
      "Epoch: 082, Test Acc: 0.8158, Test Loss: 0.0243\n",
      "Epoch: 083, Test Acc: 0.8421, Test Loss: 0.0245\n",
      "Epoch: 084, Test Acc: 0.8158, Test Loss: 0.0241\n",
      "Epoch: 085, Test Acc: 0.8421, Test Loss: 0.0240\n",
      "Epoch: 086, Test Acc: 0.8158, Test Loss: 0.0239\n",
      "Epoch: 087, Test Acc: 0.8421, Test Loss: 0.0241\n",
      "Epoch: 088, Test Acc: 0.8421, Test Loss: 0.0244\n",
      "Epoch: 089, Test Acc: 0.8421, Test Loss: 0.0242\n",
      "Epoch: 090, Test Acc: 0.8421, Test Loss: 0.0242\n",
      "Epoch: 091, Test Acc: 0.8421, Test Loss: 0.0240\n",
      "Epoch: 092, Test Acc: 0.8158, Test Loss: 0.0238\n",
      "Epoch: 093, Test Acc: 0.8421, Test Loss: 0.0238\n",
      "Epoch: 094, Test Acc: 0.8158, Test Loss: 0.0237\n",
      "Epoch: 095, Test Acc: 0.8421, Test Loss: 0.0237\n",
      "Epoch: 096, Test Acc: 0.8684, Test Loss: 0.0240\n",
      "Epoch: 097, Test Acc: 0.8421, Test Loss: 0.0237\n",
      "Epoch: 098, Test Acc: 0.8421, Test Loss: 0.0237\n",
      "Epoch: 099, Test Acc: 0.8684, Test Loss: 0.0238\n",
      "Epoch: 100, Test Acc: 0.8684, Test Loss: 0.0238\n",
      "Epoch: 101, Test Acc: 0.8684, Test Loss: 0.0237\n",
      "Epoch: 102, Test Acc: 0.8421, Test Loss: 0.0235\n",
      "Epoch: 103, Test Acc: 0.8421, Test Loss: 0.0234\n",
      "Epoch: 104, Test Acc: 0.8421, Test Loss: 0.0235\n",
      "Epoch: 105, Test Acc: 0.8684, Test Loss: 0.0236\n",
      "Epoch: 106, Test Acc: 0.8421, Test Loss: 0.0233\n",
      "Epoch: 107, Test Acc: 0.8421, Test Loss: 0.0232\n",
      "Epoch: 108, Test Acc: 0.8684, Test Loss: 0.0234\n",
      "Epoch: 109, Test Acc: 0.8421, Test Loss: 0.0232\n",
      "Epoch: 110, Test Acc: 0.8421, Test Loss: 0.0232\n",
      "Epoch: 111, Test Acc: 0.8421, Test Loss: 0.0233\n",
      "Epoch: 112, Test Acc: 0.8421, Test Loss: 0.0233\n",
      "Epoch: 113, Test Acc: 0.8684, Test Loss: 0.0233\n",
      "Epoch: 114, Test Acc: 0.8421, Test Loss: 0.0231\n",
      "Epoch: 115, Test Acc: 0.8421, Test Loss: 0.0231\n",
      "Epoch: 116, Test Acc: 0.8684, Test Loss: 0.0232\n",
      "Epoch: 117, Test Acc: 0.8684, Test Loss: 0.0233\n",
      "Epoch: 118, Test Acc: 0.8421, Test Loss: 0.0229\n",
      "Epoch: 119, Test Acc: 0.8684, Test Loss: 0.0231\n",
      "Epoch: 120, Test Acc: 0.8421, Test Loss: 0.0229\n",
      "Epoch: 121, Test Acc: 0.8421, Test Loss: 0.0228\n",
      "Epoch: 122, Test Acc: 0.8684, Test Loss: 0.0229\n",
      "Epoch: 123, Test Acc: 0.8684, Test Loss: 0.0229\n",
      "Epoch: 124, Test Acc: 0.8684, Test Loss: 0.0227\n",
      "Epoch: 125, Test Acc: 0.8421, Test Loss: 0.0225\n",
      "Epoch: 126, Test Acc: 0.8421, Test Loss: 0.0224\n",
      "Epoch: 127, Test Acc: 0.8684, Test Loss: 0.0231\n",
      "Epoch: 128, Test Acc: 0.8684, Test Loss: 0.0227\n",
      "Epoch: 129, Test Acc: 0.8421, Test Loss: 0.0224\n",
      "Epoch: 130, Test Acc: 0.8421, Test Loss: 0.0223\n",
      "Epoch: 131, Test Acc: 0.8684, Test Loss: 0.0224\n",
      "Epoch: 132, Test Acc: 0.8421, Test Loss: 0.0223\n",
      "Epoch: 133, Test Acc: 0.8684, Test Loss: 0.0229\n",
      "Epoch: 134, Test Acc: 0.8421, Test Loss: 0.0222\n",
      "Epoch: 135, Test Acc: 0.8421, Test Loss: 0.0222\n",
      "Epoch: 136, Test Acc: 0.8684, Test Loss: 0.0223\n",
      "Epoch: 137, Test Acc: 0.8684, Test Loss: 0.0224\n",
      "Epoch: 138, Test Acc: 0.8421, Test Loss: 0.0222\n",
      "Epoch: 139, Test Acc: 0.8158, Test Loss: 0.0222\n",
      "Epoch: 140, Test Acc: 0.8421, Test Loss: 0.0224\n",
      "Epoch: 141, Test Acc: 0.8684, Test Loss: 0.0227\n",
      "Epoch: 142, Test Acc: 0.8158, Test Loss: 0.0222\n",
      "Epoch: 143, Test Acc: 0.8158, Test Loss: 0.0222\n",
      "Epoch: 144, Test Acc: 0.8421, Test Loss: 0.0221\n",
      "Epoch: 145, Test Acc: 0.8684, Test Loss: 0.0222\n",
      "Epoch: 146, Test Acc: 0.8684, Test Loss: 0.0221\n",
      "Epoch: 147, Test Acc: 0.8684, Test Loss: 0.0221\n",
      "Epoch: 148, Test Acc: 0.8684, Test Loss: 0.0220\n",
      "Epoch: 149, Test Acc: 0.8684, Test Loss: 0.0220\n",
      "Epoch: 150, Test Acc: 0.8421, Test Loss: 0.0218\n",
      "Epoch: 151, Test Acc: 0.8684, Test Loss: 0.0220\n",
      "Epoch: 152, Test Acc: 0.8684, Test Loss: 0.0219\n",
      "Epoch: 153, Test Acc: 0.8684, Test Loss: 0.0218\n",
      "Epoch: 154, Test Acc: 0.8421, Test Loss: 0.0216\n",
      "Epoch: 155, Test Acc: 0.8684, Test Loss: 0.0221\n",
      "Epoch: 156, Test Acc: 0.8684, Test Loss: 0.0222\n",
      "Epoch: 157, Test Acc: 0.8158, Test Loss: 0.0217\n",
      "Epoch: 158, Test Acc: 0.8684, Test Loss: 0.0217\n",
      "Epoch: 159, Test Acc: 0.8684, Test Loss: 0.0217\n",
      "Epoch: 160, Test Acc: 0.8158, Test Loss: 0.0217\n",
      "Epoch: 161, Test Acc: 0.8684, Test Loss: 0.0219\n",
      "Epoch: 162, Test Acc: 0.8684, Test Loss: 0.0220\n",
      "Epoch: 163, Test Acc: 0.8684, Test Loss: 0.0222\n",
      "Epoch: 164, Test Acc: 0.8684, Test Loss: 0.0219\n",
      "Epoch: 165, Test Acc: 0.8158, Test Loss: 0.0218\n",
      "Epoch: 166, Test Acc: 0.8684, Test Loss: 0.0221\n",
      "Epoch: 167, Test Acc: 0.8684, Test Loss: 0.0224\n",
      "Epoch: 168, Test Acc: 0.8684, Test Loss: 0.0222\n",
      "Epoch: 169, Test Acc: 0.8684, Test Loss: 0.0219\n",
      "Epoch: 170, Test Acc: 0.8421, Test Loss: 0.0219\n",
      "Epoch: 171, Test Acc: 0.8684, Test Loss: 0.0219\n",
      "Epoch: 172, Test Acc: 0.8684, Test Loss: 0.0219\n",
      "Epoch: 173, Test Acc: 0.8684, Test Loss: 0.0223\n",
      "Epoch: 174, Test Acc: 0.8421, Test Loss: 0.0219\n",
      "Epoch: 175, Test Acc: 0.8684, Test Loss: 0.0220\n",
      "Epoch: 176, Test Acc: 0.8684, Test Loss: 0.0219\n",
      "Epoch: 177, Test Acc: 0.8684, Test Loss: 0.0219\n",
      "Epoch: 178, Test Acc: 0.8684, Test Loss: 0.0219\n",
      "Epoch: 179, Test Acc: 0.8684, Test Loss: 0.0218\n",
      "Epoch: 180, Test Acc: 0.8158, Test Loss: 0.0216\n",
      "Epoch: 181, Test Acc: 0.8421, Test Loss: 0.0216\n",
      "Epoch: 182, Test Acc: 0.8684, Test Loss: 0.0220\n",
      "Epoch: 183, Test Acc: 0.8684, Test Loss: 0.0220\n",
      "Epoch: 184, Test Acc: 0.8158, Test Loss: 0.0215\n",
      "Epoch: 185, Test Acc: 0.8421, Test Loss: 0.0216\n",
      "Epoch: 186, Test Acc: 0.8421, Test Loss: 0.0215\n",
      "Epoch: 187, Test Acc: 0.8684, Test Loss: 0.0219\n",
      "Epoch: 188, Test Acc: 0.8684, Test Loss: 0.0220\n",
      "Epoch: 189, Test Acc: 0.8684, Test Loss: 0.0216\n",
      "Epoch: 190, Test Acc: 0.8684, Test Loss: 0.0216\n",
      "Epoch: 191, Test Acc: 0.8158, Test Loss: 0.0216\n",
      "Epoch: 192, Test Acc: 0.8684, Test Loss: 0.0217\n",
      "Epoch: 193, Test Acc: 0.8684, Test Loss: 0.0220\n",
      "Epoch: 194, Test Acc: 0.8684, Test Loss: 0.0216\n",
      "Epoch: 195, Test Acc: 0.8684, Test Loss: 0.0214\n",
      "Epoch: 196, Test Acc: 0.8158, Test Loss: 0.0213\n",
      "Epoch: 197, Test Acc: 0.8684, Test Loss: 0.0214\n",
      "Epoch: 198, Test Acc: 0.8684, Test Loss: 0.0215\n",
      "Epoch: 199, Test Acc: 0.8684, Test Loss: 0.0216\n",
      "Epoch: 200, Test Acc: 0.8684, Test Loss: 0.0215\n"
     ]
    }
   ],
   "source": [
    "# Create the model\n",
    "model = SimpleGNN(dataset.num_node_features, [64, 64], 64, dataset.num_classes)\n",
    "\n",
    "# Dataloader for random sampling and batching\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Create the optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)\n",
    "\n",
    "# Create the loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Create the training loop\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        out = model(batch)\n",
    "        loss = criterion(out, batch.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Create the testing loop\n",
    "def test():\n",
    "    model.eval()\n",
    "\n",
    "    correct = 0\n",
    "    loss = 0\n",
    "    for batch in test_loader:\n",
    "        out = model(batch)\n",
    "        loss += criterion(out, batch.y).item()\n",
    "        pred = out.argmax(dim=1)\n",
    "        correct += int((pred == batch.y).sum())\n",
    "\n",
    "    return correct / len(test_loader.dataset), loss / len(test_loader.dataset)\n",
    "                                              \n",
    "# Run the training loop\n",
    "for epoch in range(1, 201):\n",
    "    train(epoch)\n",
    "    test_acc, test_loss = test()\n",
    "    print(f'Epoch: {epoch:03d}, Test Acc: {test_acc:.4f}, Test Loss: {test_loss:.4f}')\n",
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), 'model.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "selective-long-range-connection-gnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
